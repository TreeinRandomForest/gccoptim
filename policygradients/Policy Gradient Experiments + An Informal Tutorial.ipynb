{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a document that was meant to record some experiments with policy gradients which refers to a class of reinforcement learning techniques. My goal was to get better intuition and I'll transfer more results here over a few days. I am also adding some notes that will hopefully make it useful as a tutorial.\n",
    "\n",
    "Maybe this should be a latex doc with embedded code, maybe it should be a notebook, maybe it should be a video. I can't decide which one would aid understanding the most, so here is one option that might work well.\n",
    "\n",
    "It is helpful to have a few jupyter notebook extensions enabled:\n",
    "\n",
    "* Table of Contents (get floating table on left-side. makes it easy to jump around).\n",
    "\n",
    "* Collapsible Headings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive-by View\n",
    "\n",
    "Suppose, we want a computer to learn the steps to successfully achieve a task. Instead of explicitly programming the computer, we want it to figure a program out on its own. By program here, we don't necessarily mean the actual source code in a language. While we as humans use programming languages to express ideas, it's fruitful to deal directly with mathematics when a computer is learning on its own.\n",
    "\n",
    "It helps to have a few examples when dealing with a new subject. Imagine the following:\n",
    "\n",
    "* playing a game - chess, age of empires, pong etc. \n",
    "\n",
    "* debugging code\n",
    "\n",
    "* navigating a robot in a room\n",
    "\n",
    "* sorting an array\n",
    "\n",
    "The fourth one is one where we have very efficient algorithms that we can implement directly in a language of our choice. The other three are not so simple. We don't have specific rules or algorithms that we can follow to carry them out. We can try and codify what we would do in those situations but that often leads to catastrophic edge cases and algorithms that don't react well in unknown situations. At this stage, we'll abstract out the core properties of these problems.\n",
    "\n",
    "## A Closer Drive-by View\n",
    "\n",
    "One can view all these problems as decision-making processes. How do we make decisions in real life?\n",
    "\n",
    "* We assess a situation\n",
    "\n",
    "* We use what we know to decide on and carry out a decision\n",
    "\n",
    "* We observe a signal that tells us whether what we did was fruitful or not\n",
    "\n",
    "* Repeat\n",
    "\n",
    "Real life is messy but maybe it is possible to approximate all decision-making processes by the above framework. To attempt to do so, we need to make each step above more precise.\n",
    "\n",
    "**\"We assess a situation\"**:\n",
    "\n",
    "What does this mean? In the case of a game, we stare at a board or a computer screen and observe the current situation. Our observation, whether described in words or mathematical symbols or pictures, describes everything going on at this moment in time. We usually call this description the **state** of the system we are observing. It is a mathematical object called a **vector** (sorry if this is too pedantic) which you can think of as a collection of $n$ numbers.\n",
    "\n",
    "**\"We use what we know to decide on and carry out a decision\"**\n",
    "\n",
    "We observe the state and make a decision. This process can be encoded as a function that takes in the current as well as past states as inputs and outputs the probabilities of all the possible decisions or **actions** (if finite). We then choose one of these decisions.\n",
    "\n",
    "This is, of course, easier said than done. After all, this is like describing debugging as the following process: \"look at the code and fix it\". While technically true, depending on the code base, this is a hard task! What also makes it harder is that we need to take into account not just the state now but at all times in the past. \n",
    "\n",
    "We usually have a statistical model (a neural network, linear regression, a random forest) that maps the states (and their history) to the probabilities for each action. This model is called the **policy**.\n",
    "\n",
    "**\"We observe a signal that tells us whether what we did was fruitful or not\"**\n",
    "\n",
    "This is tricky. When you are playing chess and you make a move, sometimes it's obvious that it was a great move or a very bad move. But mostly, it's not that obvious. After all, there's no way to judge a move except for the eventual outcome of winning or losing.\n",
    "\n",
    "Generally, reinforcement learning problems model a **reward** at every time-step i.e. for each action. This is highly dependent on the problem and often there can be multiple choices. In the chess example, you might assign a reward of +1 if you capture your opponent's piece, -1 if your opponent captures your piece and 0 otherwise (can you think of issues with this definition if you are a chess player?).\n",
    "\n",
    "In any case, once we have a reward at every time-step, our task is not to maximize this reward. Instead we want to maximize the sum of rewards across **all** time-steps! This is a very crucial point and is what injects the notion of long-term planning into such problems. It is usually easy to do something that maximizes current reward. But that often doesn't lead to long-term reward maximization. For example, capturing pieces greedily in Chess will maximize current reward but you'll most likely lose the game to a well-trained amateur.\n",
    "\n",
    "So at this stage, we are starting to impose some structure on how we want to describe the problem to a computer.\n",
    "\n",
    "We have some task. Everything connected to the task is called the **environment**. For chess, it's the chess board. For the robot, it's the robot and the room it is navigating within.\n",
    "\n",
    "There is a vector describing the **state** of the environment. This tells us everything relevant we need to know about the environment at any point in time and it evolves or changes with time.\n",
    "\n",
    "There is an **agent** that we focus on. This might be the chess player or our robot. The agent is what we control.\n",
    "\n",
    "The agent follows a **policy** that tells it what to do in various states. \"What to do\" refers to **actions** that can be taken.\n",
    "\n",
    "The eventual goal is to find a policy that maximizes not just the immediate reward for the agent but the long-term total sum of rewards across all time.\n",
    "\n",
    "\n",
    "\n",
    "## An Even Closer Drive-by View\n",
    "\n",
    "Consider an abstract, arbitrary state space $\\mathcal{S}$. All the possible states are elements of this space. This can be discrete or continuous and can be high-dimensional too. An an example, think of an ant's position on a two-dimensional plane, $\\mathbb{R}^2$.\n",
    "\n",
    "We always start our task at some starting point, $s_0 \\in \\mathcal{S}$. This can be a fixed point or can be drawn from a distribution $\\rho(s)$.\n",
    "\n",
    "We have a task or goal to achieve. This is modelled as a reward-maximization task. The goal is to take steps or actions from an action space, $\\mathcal{A}$ (all actions belong to this set) that lead us to follow trajectories or paths that maximize some (to be made precise below) overall reward. In our example, the goal might be to go from a starting point to an end point in the shortest possible time. This can be rephrased as a maximization problem instead of a minimization problem by the usual identification $\\max_x f(x) = \\min_x (-f(x))$ (if this is not clear, draw a diagram of an arbitrary function and see what both sides mean geometrically).\n",
    "\n",
    "Our first key observation is that we keep referring to \"current time-step\". Time is continuous not discrete! But realistically, all our computing systems treat time as a discrete quantity. It might be a very fine-grained discretization but it is discrete nonetheless and hence, it makes sense to talk about \"time-steps\".\n",
    "\n",
    "So, we have time ticking along at a regular rate (not how the universe works but unless we are working with relativistic objects, it's a very good approximation). At each step, we observe the state of our environment, pick an action recommended by the policy, get an immediate reward and observe the next state. This loop repeats either indefinitely or till a natural stopping point in time. This gives us a sequence:\n",
    "\n",
    "$$s_1 \\rightarrow a_1 \\rightarrow r_1 \\rightarrow s_2 \\rightarrow a_2 \\rightarrow r_2 \\rightarrow \\ldots $$\n",
    "\n",
    "or for arbitrary time,\n",
    "\n",
    "$$\\ldots \\rightarrow s_t \\rightarrow a_t \\rightarrow r_t \\rightarrow \\ldots $$\n",
    "\n",
    "Our goal is to maximize the total reward:\n",
    "\n",
    "$$R = r_1 + r_2 + r_3 + \\ldots + r_t + \\ldots$$\n",
    "\n",
    "**Pedantic Note**:\n",
    "\n",
    "This is usually condensed using a capital-Sigma (Greek \"S\" letter, \"S\" for sum) to:\n",
    "\n",
    "$$R = \\Sigma_{t=1}^{T} r_t$$\n",
    "\n",
    "where $T$ is when the experiment ends or can be $\\infty$ if the experiment never ends. Often, it is onerous to carry around the index and it is suppressed:\n",
    "\n",
    "$$R = \\Sigma_t r_t = \\Sigma r_t$$\n",
    "\n",
    "**End of Pedantic Note**\n",
    "\n",
    "To make this more transparent, the only thing **we** (the agent and the policy running the agent) control in the sequence:\n",
    "\n",
    "$$\\ldots \\rightarrow s_t \\rightarrow a_t \\rightarrow r_t \\rightarrow \\ldots $$\n",
    "\n",
    "are the actions, $a_t$. The action at a given time-step might depend on the current state, $s_t$ as well as all the historical states, $s_1, s_2, \\ldots, s_{t-1}$. Here is where we make our second assumption. We will only, in theory at least, deal with so-called **Markov** systems. What does this mean? Very simply, it means that the state at time $t+1$ only depends on the state and action at time $t$. Nothing prior to time $t$ matters for this transition from $t \\rightarrow t+1$. \n",
    "\n",
    "What is the motivation for making this assumption? A big benefit is ease of mathematical reasoning. Often, the art of mathematically modeling a system, whether in physics or finance or biology, is to consider just enough information about the system to (1) be able to mathematically solve the equations one uses to describe the system, (2) be able to describe the system's behavior within a tolerable error.\n",
    "\n",
    "Another benefit often is that some systems have been extensively studied before and if one is justified in making an assumption that our system belongs to one of these previously-studied systems, it opens up a vast arsenal of results that we can rely upon.\n",
    "\n",
    "A quick work about Markov systems. Forget about all our discussion of reinforcement learning. Suppose, we are given a state space, $\\mathcal{S}$. The state changes in time:\n",
    "\n",
    "$$s_1 \\rightarrow s_2 \\rightarrow s_3 \\rightarrow \\ldots \\rightarrow s_t \\rightarrow \\ldots$$ \n",
    "\n",
    "The probability that we get a given state $s_{t+1}$ at time $t+1$ will depend on all the prior states:\n",
    "\n",
    "$$\\mathbb{P}[s_{t+1} | s_1, s_2, \\ldots, s_t]$$\n",
    "\n",
    "The **Markov assumption** is that this probability is actually independent of all states except $s_t$:\n",
    "\n",
    "$$\\mathbb{P}[s_{t+1} | s_1, s_2, \\ldots, s_t] = \\mathbb{P}[s_{t+1}|s_t]$$\n",
    "\n",
    "This implies that if the state space is discrete, we can write a so-called **transition matrix**, $P$ with $ij-$th element being the probability:\n",
    "\n",
    "$$\\mathbb{P}[s_{t+1}=j | s_t=i]$$\n",
    "\n",
    "At time $t+1$, we have to land in some state, so\n",
    "\n",
    "$$\\Sigma_j P_{ij} = \\Sigma_j \\mathbb{P}[s_{t+1}=j | s_t=i] = 1$$\n",
    "\n",
    "We won't discuss Markov processes (also called Markov chains) in more detail here.\n",
    "\n",
    "Going back to reinforcement learning, this assumption translates into the idea that the state at time $t+1$ only depends on the state at time $t$ and the action at time $t$. As we will see later, this is a crucial assumption to make progress with policy gradients. At the same time, one should naturally ask if this is a valid assumption. Are most real systems Markov? Our intuition might tell us that what happens in real-systems depends on the history the system went through and not just what happened at the previous time-step.\n",
    "\n",
    "The real answer is that we rarely know at the outset if a system is Markov or not. We always have the freedom to re-define the definition of state: $s'_t = (s_t, s_{t-1})$ in which case a new state $s'$ concatenates the old state at two (or more) time-steps! A concrete example is that of Atari video-games like Pong. Instead of training an agent by making it look at just one frame of the game (in which case one loses all information about the velocity of the ball), one takes the difference of two consecutive frames to define a new frame (or state).\n",
    "\n",
    "Going back to the sequence:\n",
    "\n",
    "$$\\ldots \\rightarrow s_t \\rightarrow a_t \\rightarrow r_t \\rightarrow \\ldots $$\n",
    "\n",
    "the only element we (the agent) control is $a_t$ chosen by the policy. Practically, the policy is represented by a statistical model, denoted $\\pi(a_t|s_t, \\theta)$ or $\\pi_{\\theta}(a_t|s_t)$. You should imagine a linear regression or neural network here (as a couple of examples). $\\theta$ denotes the set of parameters - coefficients for linear regression and weights and biases for neural networks - that need to be tuned and chosen. The notation $(a_t | s_t)$ is standard probability notation to be read as \"a_t given s_t\". In other words, $\\pi_{\\theta}(a_t|s_t)$ is a model that predicts the probability distribution of different actions that can be taken at time $t$ given knowledge about the state at time $t$.\n",
    "\n",
    "Our final goal then is to choose $\\theta$ such that if $\\pi_{\\theta}(a_t|s_t)$ is used to pick actions, then **on average**, we will get sequences or trajectories:\n",
    "\n",
    "$$\\ldots \\rightarrow s_t \\rightarrow a_t \\rightarrow r_t \\rightarrow \\ldots $$\n",
    "\n",
    "that lead to maximal overall reward:\n",
    "\n",
    "$$R = r_1 + r_2 + \\ldots$$\n",
    "\n",
    "## Finer Points\n",
    "\n",
    "(not relevant for now)\n",
    "\n",
    "gamma\n",
    "\n",
    "time\n",
    "\n",
    "state space\n",
    "\n",
    "action space\n",
    "\n",
    "reward structure\n",
    "\n",
    "MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we discussed reinforcement learning, we didn't discuss how we would actually find the optimal policy, $\\pi(a_t|s_t, \\theta)$. **Policy gradient methods** describe a general set of techniques to directly change the policy to maximize the expected (more on this) reward.\n",
    "\n",
    "Suppose, we consider the space of all trajectories:\n",
    "\n",
    "$$\\tau \\equiv s_1 \\rightarrow a_1 \\rightarrow r_1 \\rightarrow s_2 \\rightarrow a_2 \\rightarrow r_2 \\rightarrow \\ldots $$\n",
    "\n",
    "The only element we control are the actions $a_t$ through the policy $\\pi$. In principle, we could run across every single trajectory, measure its total reward and store it in a gigantic table. Then, given a start state $s_1$, we would look at all trajectories starting from $s_1$, and pick the action from the trajectory with the highest reward (from the table). This process can be repeated as long as we have the table. Of course, this is prohibitive because (1) even if the state and action spaces are discrete, the number of trajectories is huge!, (2) since our actions are stochastic, we can't actually guarantee that a certain action will lead to a certain state.\n",
    "\n",
    "What do we do instead? Each trajectory has a certain total reward associated with it. Every time we run with a certain policy i.e. use a policy to choose the actions $a_t$, we get one sample trajectory. We can't guarantee that there is the best or optimal policy that always puts us on the trajectory with the maximum reward but we can try aiming to get the maximal average (or expected) reward. As an analogy, if you are a world-class tennis player, we can't guarantee a training regime that will ensure you win every game you ever play. There is some randomness to that process (you have a bad day, you are injured etc.) but we can at least aim to ensure that on average your likelihood to win is as high as possible.\n",
    "\n",
    "In mathematical terms, the average or expected total reward is given by:\n",
    "\n",
    "$$J \\equiv \\Sigma_{\\tau} R(\\tau) P(\\tau; \\theta)$$\n",
    "\n",
    "**Pedantic Aside**:\n",
    "\n",
    "An average or mean of n numbers is simply:\n",
    "\n",
    "$$\\bar{x} = \\frac{x_1 + \\ldots + x_n}{n}$$\n",
    "\n",
    "Imagine we have k distinct values, $x_1, x_2, \\ldots, x_k$ each occuring $n_1, n_2, \\ldots, n_k$ times. We have $n_1 + \\ldots + n_k = n$.\n",
    "\n",
    "We can rewrite the average as:\n",
    "\n",
    "$$\\bar{x} = \\frac{x_1 n_1 + x_2 n_2 + \\ldots + x_k n_k}{n} = x_1 p_1 + \\ldots + x_k p_k$$\n",
    "\n",
    "where $p_i = \\frac{n_i}{n}$ is the probability of getting a measurement, $x_i$. The continuous version of this is:\n",
    "\n",
    "$\\bar{x} = \\int dx x p(x)$\n",
    "\n",
    "where $p(x)$ is a probability distribution function (p.d.f.) i.e. $p(x) \\geq 0$ and $\\int dx p(x) = 1$.\n",
    "\n",
    "Lastly, the average or expected value of a function of the inputs would be:\n",
    "\n",
    "$$\\bar{f(X)} = \\mathbb{E}f(X) \\equiv \\Sigma_{x} f(x)p(x)$$\n",
    "\n",
    "where $\\mathbb{E}$ denotes the expected value. In this notation, we have\n",
    "\n",
    "$$J = \\mathbb{E}R = \\Sigma_{\\tau} R(\\tau) P(\\tau; \\theta)$$\n",
    "\n",
    "**End of aside**\n",
    "\n",
    "This equation packs a lot of meaning so let's decode it. We are summing over all trajectories, $\\tau$. That is computationally infeasible but that's not relevant right now. $R(\\tau)$ denotes the total reward for some trajectory:\n",
    "\n",
    "$$\\tau \\equiv s_1 \\rightarrow a_1 \\rightarrow r_1 \\rightarrow s_2 \\rightarrow a_2 \\rightarrow r_2 \\rightarrow \\ldots $$\n",
    "\n",
    "$$\\implies R(\\tau) = r_1 + r_2 + r_3 + \\ldots$$\n",
    "\n",
    "$P(\\tau; \\theta)$ denotes the total probability of a trajectory. This is where the Markov property is very useful. Because each time-step depends only on the previous one, we can treat different times as independent and write:\n",
    "\n",
    "$$P(\\tau) = \\underbrace{{\\rho(s_1)}}_{\\text{initial dist.}} \\underbrace{\\pi(a_1|s_1)}_{\\text{policy}}\\underbrace{\\rho(s_2|s_1,a_1)}_{\\text{dynamics}} \\underbrace{\\pi(a_2|s_2)}_{\\text{policy}} \\underbrace{\\rho(s_3|s_2,a_2)}_{\\text{dynamics}}\\ldots$$\n",
    "\n",
    "Each term above is a probability. The policy tells us the probabilities of taking various actions. Dynamics are something intrinsic to the system. $\\rho(s_t|s_{t-1},a_{t-1})$ describes how a system evolves in time or how its state changes with time. This is exactly what a lot of physics is all about. We want to discover how planets will orbit with time or how a hydrogen atom will behave over time. If something is known about the dynamics of our environment, we can and maybe even should (depending on how complex it is) exploit it. But we will generally assume that nothing is known about the dynamics a priori.\n",
    "\n",
    "We want to maximize, $J = \\Sigma_{\\tau} R(\\tau) P(\\tau; \\theta)$ but (1) we can't sum over all trajectories and (2) we don't know the initial distribution $\\rho(s_1)$ as well as the dynamics $\\rho(s_t|s_{t-1}, a_{t-1})$. So how we do make progress?\n",
    "\n",
    "The first key observation is that the only thing we control is $\\theta$. These are the parameters of the policy. We can't control the dynamics because they are intrinsic to the environment/system. A corollary is that since we control $\\theta$ (the policy's parameters), we are maximizing a quantity, $J = \\Sigma_{\\tau} R(\\tau) P(\\tau; \\theta)$ by purely adjusting $P(\\tau; \\theta)$ i.e. the probability distribution over the trajectories. Our policy's job is to ensure we are far more likely to lie on the high-reward trajectories than the low-reward ones.\n",
    "\n",
    "The second observation comes from a straightforward choice. We have a function $J(\\theta)$ that we want to maximize. If we can take the first derivatives with respect to $\\theta$, then we could use gradient ascent.\n",
    "\n",
    "**Aside**:\n",
    "\n",
    "At its core, gradient descent/ascent is remarkably simple. Suppose you have a function, $f(x)$ of one variable. Also, suppose it is differentiable. Then we can see from the following picture:\n",
    "\n",
    "![](media/gradientdescent.svg)\n",
    "\n",
    "that we should always move opposite to the direction of the derivative. If the derivative is positive, we should move in the negative direction to decrease the function. If the derivative is negative, we should move in the positive direction to decrease the function.\n",
    "\n",
    "So, we can write an iterative algorithm where we start with some initial guess $x_0$ and then update it according to:\n",
    "\n",
    "$$x_t = x_{t-1} - \\frac{df}{dx}(x_{t-1})$$\n",
    "\n",
    "Usually we want to control how far we move and that's done by introducing a step-size or learning rate, $\\eta$.\n",
    "\n",
    "$$x_t = x_{t-1} - \\eta \\frac{df}{dx}(x_{t-1})$$\n",
    "\n",
    "Another way of stating this is to say that we are locally approximating our function as a linear function (a line) and moving along the line. The further we move away from our current point, the worse the linear approximation will get and so we want to control how far we move.\n",
    "\n",
    "A key point here is that since we are only exploiting local (derivative) information, we will most likely get stuck in a local minimum not the global minimum. In general, there is no information in the derivative at a point about the global behavior of the function.\n",
    "\n",
    "For functions that depend on many variables, $f(x_1, x_2, \\ldots, x_n)$, there is a very straightforward generalization.\n",
    "\n",
    "$$f(x_1^{t}, x_2^{t}, \\ldots, x_i^{t}, \\ldots, x_n^{t}) = f(x_1^{t-1}, x_2^{t-1}, \\ldots, x_i^{t-1}, \\ldots, x_n^{t}) - \\eta \\frac{\\partial f}{\\partial x_i}(x_1^{t-1}, x_2^{t-1}, \\ldots, x_i^{t-1}\\ldots, x_n^{t-1})$$\n",
    "\n",
    "These partial derivatives form a vector called the gradient:\n",
    "\n",
    "$$\\nabla f(x_1,\\ldots,x_n) = (\\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n})$$\n",
    "\n",
    "and we write gradient descent in the more compact notation:\n",
    "\n",
    "$$\\vec{x}^{t} = \\vec{x}^{t-1} - \\eta \\nabla f(\\vec{x}^{t-1})$$\n",
    "\n",
    "Geometrically, we have some curved surface (\"manifold\") which we are approximation locally as a plane. The gradient vector points in the direction of maximal increase and so we move along/opposite the gradient by a step-size $\\eta$ for gradient ascent/descent.\n",
    "\n",
    "**End of aside**\n",
    "\n",
    "To use gradient descent, we need the gradient of $J$ with respect to $\\theta$ (which is generally a vector i.e. we have many weights that our policy depends on).\n",
    "\n",
    "So,\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\nabla \\Sigma_{\\tau} R(\\tau) P(\\tau; \\theta)$$\n",
    "\n",
    "Derivatives are linear operations i.e. $\\frac{d}{dx}(f(x) + g(x)) = \\frac{df}{dx} + \\frac{dg}{dx}$ so the derivative of the sum is the sum of derivatives:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\Sigma_{\\tau} \\nabla[R(\\tau) P(\\tau; \\theta)]$$\n",
    "\n",
    "$R(\\tau)$ doesn't depend on $\\theta$. Once the sequence of states and actions (which describes the trajectory) is fixed, changing $\\theta$ won't change the total reward, $R(\\tau)$. So,\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\Sigma_{\\tau} R(\\tau) \\nabla P(\\tau; \\theta)$$\n",
    "\n",
    "So far so good. Now it is generally ideal to write quantities as expected values because as we will see soon, averages can be estimated from samples which can be generated by Monte-Carlo simulations. Expected values have the structure $\\mathbb{E}f(X) = \\Sigma f(x) p(x)$.\n",
    "\n",
    "We instead have $\\nabla P(\\tau; \\theta)$. Can we write this as $P(\\tau; \\theta) * \\text{something}$? The analog of this would be the question:\n",
    "\n",
    "Can you write $\\frac{df}{dx}$ as $f(x) * \\text{something}$? Let's try this out.\n",
    "\n",
    "Suppose, $\\frac{df}{dx} = G(x) f(x)$ for some unknown $G(x)$. Then,\n",
    "\n",
    "$$\\frac{df}{f(x)} = G(x) dx$$\n",
    "\n",
    "Integrating both sides, we get\n",
    "\n",
    "$\\log|f(x)| = \\int G(x) dx$\n",
    "\n",
    "or (using the fundamental theorem of calculus),\n",
    "\n",
    "$$G(x) = \\frac{d\\log|f(x)|}{dx}$$\n",
    "\n",
    "Of course, you could have just guessed this from above or verified it knowing that:\n",
    "\n",
    "$$\\frac{d\\log |f(x)|}{dx} = \\frac{1}{|f(x)|} \\frac{df}{dx}$$\n",
    "\n",
    "The same calculation holds for the gradient and we get:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\Sigma_{\\tau} R(\\tau) \\nabla [\\log P(\\tau; \\theta)] P(\\tau; \\theta)$$\n",
    "\n",
    "Now we can write this as an expectation:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\nabla\\log P(\\tau; \\theta)$$\n",
    "\n",
    "where $\\mathbb{E}_{\\tau}$ makes it explicit that the average is taken over all trajectories. Why did we go through this trouble of writing our sum as an expectation? In its original form:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\Sigma_{\\tau} R(\\tau) \\nabla P(\\tau; \\theta)$$\n",
    "\n",
    "it wasn't obvious at all that we could compute this. How are we supposed to get $\\nabla P(\\tau)$? By writing it as an expectation, we can use use the machinery of Monte-Carlo to estimate the expected value i.e. $\\nabla J(\\theta)$.\n",
    "\n",
    "### Monte-Carlo Simulations\n",
    "\n",
    "Consider the problem of computing an expected value of some quantity, $X$ following a probability distribution function (p.d.f.) $p(x)$. Mathematically, we want to compute the integral:\n",
    "\n",
    "$$I = \\int_{-\\infty}^{\\infty} dx p(x) f(x)$$\n",
    "\n",
    "The problem is that we don't know $p(x)$! If we did, maybe we could do some kind of discrete approximation:\n",
    "\n",
    "$$I \\approx \\Sigma_{x_i} p(x_i) f(x_i)$$\n",
    "\n",
    "where $x_i = i * \\delta$ for some fixed lattice spacing $\\delta$ on the one-dimensional real line and $i$ is an integer.\n",
    "\n",
    "But an elegant way presents itself. What if we can **sample** various values of X?! If the samples are drawn from the distribution $p(x)$ (the histogram of $n$ samples gets closer to $p(x)$ as $n$ gets larger) and we have a way of drawing more samples when we desire, then we can approximate $I$ as:\n",
    "\n",
    "$$\\hat{I} \\approx \\Sigma_{x\\sim p} f(x_i)$$\n",
    "\n",
    "This might be confusing if it's the first time you are seeing this so let's pick an example. Suppose you want to measure the average height of people in a country, C. This would be:\n",
    "\n",
    "$$\\bar{h} \\equiv \\mathbb{E}H = \\int_0^{\\infty} dh h p(h)$$\n",
    "\n",
    "where $p(h)dh$ = probability that a person has height between $h$ and $h+dh$.\n",
    "\n",
    "We don't know $p(h)$ a priori but we just sample i.e. randomly pick $n$ people from the population. As long as the sample is random, the probabilility distribution of our sample will be similar to the actual distribution $p(h)$. BETTER EXPLANATION\n",
    "\n",
    "There are some core questions one would want to answer. (1) What is the error between the Monte-Carlo (MC) estimate, $\\hat{I}$ and the actual value, $I$. We expect this error to vanish (go to 0) as the sample size $n$ increases but how fast does it vanish? (2) What is the variance of $\\hat{I}$? If we picked $k$ different samples of size $n$ each, each sample would have a different estimate $\\hat{I}_k$. Hopefully, on average, these $I_{k}$ will be equal to the true value, $I$ (zero bias) but what about the variance in the values $I_k$? Can we minimize that?\n",
    "\n",
    "Monte-Carlo Methods are a beautiful and deep topic that have multiple applications. We won't go any deeper here but further study of the topic is highly recommended.\n",
    "\n",
    "Recall, our central object of interest was:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\nabla\\log P(\\tau; \\theta)$$\n",
    "\n",
    "If we can compute this gradient, we can use gradient ascent (not descent) to maximize (not minimize) $J(\\theta)$. Since we wrote it as an expectation, we might be able to use Monte-Carlo sampling which doesn't require the explicit knowledge of the underlying probability distribution to generate $n$ trajectories (also known as **rollouts**) from our policy and then estimate $\\nabla_{\\theta} J(\\theta)$. But we still have a problem. Unlike the simple MC example given above where $f(x_i)$ didn't depend on $p(x)$, here $f(X) = R(\\tau) \\log P(\\tau; \\theta)$ i.e. it depends on $p(x)$. So maybe we are really stuck. We don't know $P(\\tau)$ and need it to estimate the gradient even by MC methods.\n",
    "\n",
    "Let's look at $\\log P(\\tau)$ in more detail. Recall:\n",
    "\n",
    "$$P(\\tau) = \\underbrace{{\\rho(s_1)}}_{\\text{initial dist.}} \\underbrace{\\pi(a_1|s_1)}_{\\text{policy}}\\underbrace{\\rho(s_2|s_1,a_1)}_{\\text{dynamics}} \\underbrace{\\pi(a_2|s_2)}_{\\text{policy}} \\underbrace{\\rho(s_3|s_2,a_2)}_{\\text{dynamics}}\\ldots$$\n",
    "\n",
    "Logs have the nice property of mapping products to sums i.e.: $\\log(ab) = \\log(a) + \\log(b)$.\n",
    "\n",
    "$$\\log P(\\tau) = \\log\\rho(s_1)+ \\log \\pi(a_1|s_1) + \\log\\rho(s_2|s_1,a_1) + \\log\\pi(a_2|s_2) + \\log\\rho(s_r|s_2,a_1) + \\ldots$$\n",
    "\n",
    "Note that the only terms that depend on $\\theta$ are the ones with the policy, $\\pi$! The derivatives with respect to $\\theta$ for the other terms are 0! So,\n",
    "\n",
    "$$\\nabla_{\\theta}\\log P(\\tau) = \\nabla_{\\theta}\\log\\rho(s_1)+ \\nabla_{\\theta}\\log \\pi(a_1|s_1) + \\nabla_{\\theta}\\log\\rho(s_2|s_1,a_1) + \\nabla_{\\theta}\\log\\pi(a_2|s_2) + \\nabla_{\\theta}\\log\\rho(s_r|s_2,a_1) + \\ldots$$\n",
    "\n",
    "$$\\implies \\nabla_{\\theta}\\log P(\\tau) = \\nabla_{\\theta}\\log \\pi(a_1|s_1) + \\nabla_{\\theta}\\log\\pi(a_2|s_2) + \\ldots$$\n",
    "\n",
    "I am not using $\\Sigma$ explicitly to keep the notation out of the way in case the reader is just getting familiar with it. This is usually written as:\n",
    "\n",
    "$$\\implies \\nabla_{\\theta}\\log P(\\tau) = \\Sigma_{i=1}^{T}\\nabla_{\\theta}\\log \\pi(a_i|s_i)$$\n",
    "\n",
    "This is a key breakthrough. The structure of $P(\\tau)$ depended on the initial distribution of states, $\\rho(s)$ as well as on the dynamics, $\\rho(s_t|s_{t-1},a_{t-1})$ which are both generally unknown. It would be a tall order to find out the exact dynamics of an arbitrary environment. Maybe we could empirically approximate it by doing many experiments. But what we found is that we actually don't need dynamics to compute the gradient needed for gradient ascent. All we need is the policy which is completely under our control.\n",
    "\n",
    "Now we have:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\nabla \\log P(\\tau; \\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\left(\\Sigma_{t=1}^{T} \\nabla\\log \\pi(a_t|s_t)\\right) = \\boxed{\\mathbb{E}_{\\tau} \\left(\\Sigma_{t=1}^{T} r_t\\right) \\left(\\Sigma_{t=1}^{T} \\nabla\\log \\pi(a_t|s_t)\\right)}$$\n",
    "\n",
    "where we used $R(\\tau) = r_1 + r_2 + \\ldots + r_T = \\Sigma_{t=1}^T r_t$.\n",
    "\n",
    "At this stage, we can use Monte-Carlo to estimate this expected value. Just like we estimated the average height, $\\mathbb{E}H \\approx \\frac{h_1 + \\ldots + h_n}{n}$ by sampling $n$ random people, we can estimate $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau} \\left(\\Sigma_{t=1}^{T} r_t\\right) \\left(\\Sigma_{t=1}^{T} \\nabla\\log \\pi(a_t|s_t)\\right)$ by sampling $n$ random *trajectories* using actions taken from our policy. In other words,\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} \\left(\\Sigma_{t=1}^{T} r_t^{(i)}\\right) \\left(\\Sigma_{t=1}^{T} \\nabla\\log \\pi(a_t^{(i)}|s_t^{(i)})\\right)$$\n",
    "\n",
    "The index $i$ runs over the $n$ sampled trajectories. There is a superscript on each state and action indicating which trajectory it comes from. If this is too confusing, consider one trajectory. We can start in a start state (either fixed or from a chosen distribution). Our current policy (with parameters $\\theta$) gives us the probability distribution on the possible actions one can take from the current state. We sample one of these actions which \"takes us to a new state\" (more on this in a moment) and gives us an immediate reward, $r_1$. We can again use our policy to sample an action and repeat this process for time-period $T$. This is one sampled trajectory.\n",
    "\n",
    "We can compute the total reward for this trajectory from the $r_t$. Since we know what actions we took and since we know the probabilities of taking those actions from our policy, we can compute $\\log \\pi_{\\theta}(a_t|s_{t-1})$ and thus the derivatives of this with respect to $\\theta$: $\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t|s_{t-1})$. Repeating this for each time-step gives us the quantity: $\\left(\\Sigma_{t=1}^T r_t\\right)\\left(\\Sigma_{t=1}^T \\nabla_{\\theta}\\log\\pi(a_t|s_t)\\right)$\n",
    "\n",
    "We can do this for $n$ trajectories, average these products of sums, and get an estimate of $\\nabla_{\\theta} J(\\theta)$. This would be used to update our policy:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_{t} + \\eta \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "where $\\theta_{1}$ is the set of parameters for the initial policy, which is updated through time.\n",
    "\n",
    "Above, we mentioned that taking an action \"takes us to a new state\". Recall that the new state depends on the dynamics, $\\rho(s_t|s_{t-1},a_{t-1})$ which we usually don't know! So how do we know which state to go to? We usually do these experiments either in real-life or a simulator. In real-life, the dynamics are of course, \"built in\". An accurate simulator will mimic real-life closely but with the advantage that one has the ability to morph and change the environment as well as look at information not available to the policy but that might help understand the agent's behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Number Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from profilehooks import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test out these ideas, we need to start with simple experiments. Let's start with a simple state space - it is (1) discrete, (2) one-dimensional, (3) small i.e. has only a few states. We consider all integers from some limit -N to +N (inclusive of end points) to form our state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_space(N):\n",
    "    space = torch.arange(-N,N+1).float()\n",
    "    \n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n",
      "          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "space = create_state_action_space(10)\n",
    "print(space)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, [0]*len(space), 'p')\n",
    "plt.title('State Space, $\\mathcal{S}$')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel(\"Axis doesn't mean anything!\")\n",
    "_ = plt.xticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an action space. Well, let's pick it to have the same values (you'll see why) as the state space. Mathematically, the action space $\\mathcal{A}$ is completely separate from the state space, $\\mathcal{S}$ but in our case, we'll use the same function above to create the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xes, yes = torch.meshgrid(space, space)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xes.flatten(), yes.flatten(), 'p')\n",
    "\n",
    "plt.title('State-Action Space, $\\mathcal{S} x \\mathcal{A}$')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel(\"Actions\")\n",
    "\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every point in the figure above denotes a **state-action** pair: $(s,a)$. The action space happens to look the same as the state space but that's generally not the case.\n",
    "\n",
    "To define the environment and the task further, we need a **reward** function. In our case, this is a function of the state and action:\n",
    "\n",
    "$$r(s,a)$$\n",
    "\n",
    "We will define this as follows:\n",
    "\n",
    "$$r(s,a) = -(s-a)^2$$\n",
    "\n",
    "Your overwhelming question should be why this reward? Recall that we are trying to construct a toy example to try policy gradients. The state and action space were chosen to be the same. What this reward function says is that the immediate reward will be high when $s=a$ and decreases (becomes more negative) as $a$ differs more from $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xes, yes = torch.meshgrid(space, space)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xes.flatten(), yes.flatten(), 'p')\n",
    "plt.plot(space, space, 'p', color='orange')\n",
    "\n",
    "plt.title('Rewards highest along diagonal, $s=a$')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel(\"Actions\")\n",
    "\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing missing from our example is the dynamics which is defined below. The dynamics depends on the system under study. Since we are building an artificial system, we can choose any dynamics we like.\n",
    "\n",
    "At time $t$, given the state $s_t$ and the chosen action $a_t$, we define the dynamics to be:\n",
    "\n",
    "$$s_{t+1} = s_t + k a_t$$\n",
    "\n",
    "where $k$ is some fixed step-size (=1 in all our experiments).\n",
    "\n",
    "Since the states are restricted to a finite range $-L$ to $L$ (L=10 above), what happens if $s_t + ka_t > L$ or $s_t + ka_t < -L$ which are not valid states? In these cases, we use **periodic boundary conditions** where the states wrap around. In other words, the value $L+1$ (11) corresponds to $-L$ (-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_start_state(state_space):\n",
    "    '''Choose a start state uniformly randomly from state space\n",
    "    '''\n",
    "    state = state_space[torch.multinomial(torch.ones(len(state_space)), 1)]\n",
    "\n",
    "    return state\n",
    "\n",
    "#some useful state parameters\n",
    "N_space = len(space)\n",
    "low_state = torch.min(space).unsqueeze(0).unsqueeze(0)\n",
    "high_state = torch.max(space).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "def dynamics(state, action, step_size, space):\n",
    "    '''Simple dynamics\n",
    "    '''\n",
    "    if not isinstance(step_size, int) or step_size <= 0:\n",
    "        raise ValueError(\"step_size should be a positive integer\")\n",
    "\n",
    "    #move left/right depending on action\n",
    "    #space was chosen to have negative values so action could be both positive (right moves)\n",
    "    #or negatives (left moves)\n",
    "    new_state = state + step_size*action\n",
    "\n",
    "    #hard boundary conditions - if hit boundary, stay there\n",
    "    #if new_state > high_state:\n",
    "    #    new_state = high_state\n",
    "    #if new_state < low_state:\n",
    "    #    new_state = low_state\n",
    "\n",
    "    #periodic boundary conditions\n",
    "    new_state = (new_state - low_state) % N_space + low_state\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "def reward(state, action):\n",
    "    return -torch.pow((state - action), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define our policy, $\\pi(a_t|s_t)$. This can be any differentiable model but we'll choose a neural network here. N_inputs is the dimensionality of the state space and N_outputs is the number of distinct (discrete, in our case) actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, N_inputs, N_outputs, N_hidden_layers, N_hidden_nodes, activation, output_activation):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.N_inputs = N_inputs\n",
    "        self.N_outputs = N_outputs\n",
    "        \n",
    "        self.N_hidden_layers = N_hidden_layers\n",
    "        self.N_hidden_nodes = N_hidden_nodes\n",
    "        \n",
    "        self.layer_list = nn.ModuleList([]) #use just as a python list\n",
    "        for n in range(N_hidden_layers):\n",
    "            if n==0:\n",
    "                self.layer_list.append(nn.Linear(N_inputs, N_hidden_nodes))\n",
    "            else:\n",
    "                self.layer_list.append(nn.Linear(N_hidden_nodes, N_hidden_nodes))\n",
    "        \n",
    "        self.output_layer = nn.Linear(N_hidden_nodes, N_outputs)\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = inp\n",
    "        for layer in self.layer_list:\n",
    "            out = layer(out)\n",
    "            out = self.activation(out)\n",
    "            \n",
    "        out = self.output_layer(out)\n",
    "        if self.output_activation is not None:\n",
    "            pred = self.output_activation(out)\n",
    "        else:\n",
    "            pred = out\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to start generating trajectories or paths! The process is very simple. We select a starting state (either deterministically or randomly), use our policy to generate a probability distribution on the actions, sample an action (either pick the one with maximum probability or sample according to the distribution), get an immediate reward and use the dynamics to jump to a new state. This process is repeated on the new state till some termination criterion. In our case, we run each trajectory for $T$ time-steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectories(N, T, policy, step_size, space, debug=False):\n",
    "    traj_action_probs = []\n",
    "    traj_states = []\n",
    "    traj_actions = []\n",
    "    traj_rewards = []\n",
    "    traj_total_rewards = []\n",
    "\n",
    "    for i in range(N): #loop over trajectories\n",
    "        \n",
    "        #select initial state for trajectory\n",
    "        state = select_start_state(space).unsqueeze(0)\n",
    "        \n",
    "        #needed quantities\n",
    "        action_probs_list = [] #log probability of each action taken\n",
    "        reward_list = [] #each immediate reward received\n",
    "        state_list = [] #record each state\n",
    "        action_list = [] #record each action\n",
    "        total_reward = 0 #total reward\n",
    "\n",
    "        if debug:\n",
    "            print(f'Starting State: {state}')\n",
    "        \n",
    "        for t in range(T): #loop over time-steps within a trajectory\n",
    "            \n",
    "            #use policy to compute probability distribution on action space\n",
    "            action_probs = policy(state).squeeze(0)\n",
    "            \n",
    "            assert((action_probs.sum().item()-1)**2 < 0.01)\n",
    "            \n",
    "            #select one of the actions according to the probability distribution\n",
    "            action_selected_index = torch.multinomial(action_probs, 1) #index in action list\n",
    "            action_selected_prob = action_probs[action_selected_index] #probability value\n",
    "            action_selected = space[action_selected_index] #action selected (in our action space)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'---------Time step: {t}---------')\n",
    "                print(f'Action probabilities: {action_probs}')\n",
    "                print(f'Sum of Action probabilities: {action_probs.sum().item()}')\n",
    "                print(f'Action selected index: {action_selected_index}')\n",
    "                print(f'Action selected prob : {action_selected_prob}')\n",
    "                print(f'Action selected      : {action_selected}')\n",
    "\n",
    "            #record action prob (needed for computing gradients)\n",
    "            action_probs_list.append(action_selected_prob)\n",
    "            action_list.append(action_selected)\n",
    "            \n",
    "            #get immediate reward\n",
    "            r = reward(state, action_selected).squeeze(0)\n",
    "            reward_list.append(r)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'Reward for State {state} Action {action_selected}: {r}')\n",
    "\n",
    "            #use dynamics to jump to next state\n",
    "            state_list.append(state)\n",
    "            state = dynamics(state, action_selected, step_size, space)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'New State: {state}')\n",
    "            #add check: if state not in space\n",
    "\n",
    "            #total reward received till now\n",
    "            total_reward += r\n",
    "            \n",
    "            if debug:\n",
    "                print(f'Total reward accumulated: {total_reward}')\n",
    "                print('---------------\\n')\n",
    "\n",
    "        #get data for each trajectory\n",
    "        traj_action_probs.append(action_probs_list)\n",
    "        traj_rewards.append(reward_list)\n",
    "        traj_actions.append(action_list)\n",
    "        traj_states.append(state_list)\n",
    "        traj_total_rewards.append(total_reward)\n",
    "\n",
    "    return {\n",
    "            'action_probs': traj_action_probs,\n",
    "            'rewards': traj_rewards,\n",
    "            'states': traj_states, \n",
    "            'actions': traj_actions,\n",
    "            'total_rewards': traj_total_rewards\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a trajectory, we need to initialize our policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize policy\n",
    "\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 1 trajectory with 10 time-steps using our policy above. debug=True prints out details of each time-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting State: tensor([[0.]])\n",
      "---------Time step: 0---------\n",
      "Action probabilities: tensor([0.0464, 0.0502, 0.0246, 0.0423, 0.0583, 0.0415, 0.0645, 0.0382, 0.0796,\n",
      "        0.0408, 0.0597, 0.0509, 0.0429, 0.0359, 0.0409, 0.0405, 0.0315, 0.0308,\n",
      "        0.0608, 0.0533, 0.0666], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999998807907104\n",
      "Action selected index: tensor([8])\n",
      "Action selected prob : tensor([0.0796], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-2.])\n",
      "Reward for State tensor([[0.]]) Action tensor([-2.]): tensor([-4.])\n",
      "New State: tensor([[-2.]])\n",
      "Total reward accumulated: tensor([-4.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 1---------\n",
      "Action probabilities: tensor([0.0791, 0.0302, 0.0143, 0.0241, 0.0586, 0.0485, 0.0366, 0.0227, 0.0667,\n",
      "        0.0519, 0.0574, 0.0423, 0.0733, 0.0158, 0.0613, 0.0521, 0.0397, 0.0635,\n",
      "        0.0539, 0.0582, 0.0501], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([20])\n",
      "Action selected prob : tensor([0.0501], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([10.])\n",
      "Reward for State tensor([[-2.]]) Action tensor([10.]): tensor([-144.])\n",
      "New State: tensor([[8.]])\n",
      "Total reward accumulated: tensor([-148.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 2---------\n",
      "Action probabilities: tensor([4.5668e-03, 4.2685e-02, 2.7030e-03, 8.3183e-02, 7.8921e-02, 4.5285e-03,\n",
      "        8.3776e-03, 7.2841e-03, 3.8616e-03, 5.1410e-03, 3.9719e-01, 6.1997e-03,\n",
      "        2.1531e-04, 5.4157e-03, 1.4451e-04, 7.9339e-04, 6.2988e-02, 2.4993e-03,\n",
      "        1.7227e-03, 2.8042e-01, 1.1643e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([19])\n",
      "Action selected prob : tensor([0.2804], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([9.])\n",
      "Reward for State tensor([[8.]]) Action tensor([9.]): tensor([-1.])\n",
      "New State: tensor([[-4.]])\n",
      "Total reward accumulated: tensor([-149.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 3---------\n",
      "Action probabilities: tensor([0.1056, 0.0178, 0.0042, 0.0132, 0.0598, 0.0444, 0.0145, 0.0101, 0.0690,\n",
      "        0.0436, 0.0586, 0.0224, 0.0946, 0.0041, 0.0640, 0.0591, 0.0495, 0.1110,\n",
      "        0.0363, 0.0840, 0.0342], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([18])\n",
      "Action selected prob : tensor([0.0363], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([8.])\n",
      "Reward for State tensor([[-4.]]) Action tensor([8.]): tensor([-144.])\n",
      "New State: tensor([[4.]])\n",
      "Total reward accumulated: tensor([-293.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 4---------\n",
      "Action probabilities: tensor([0.0202, 0.0741, 0.0137, 0.0971, 0.0928, 0.0217, 0.0338, 0.0274, 0.0282,\n",
      "        0.0225, 0.2100, 0.0257, 0.0047, 0.0245, 0.0040, 0.0100, 0.0709, 0.0125,\n",
      "        0.0159, 0.1755, 0.0151], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999998807907104\n",
      "Action selected index: tensor([10])\n",
      "Action selected prob : tensor([0.2100], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([0.])\n",
      "Reward for State tensor([[4.]]) Action tensor([0.]): tensor([-16.])\n",
      "New State: tensor([[4.]])\n",
      "Total reward accumulated: tensor([-309.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 5---------\n",
      "Action probabilities: tensor([0.0202, 0.0741, 0.0137, 0.0971, 0.0928, 0.0217, 0.0338, 0.0274, 0.0282,\n",
      "        0.0225, 0.2100, 0.0257, 0.0047, 0.0245, 0.0040, 0.0100, 0.0709, 0.0125,\n",
      "        0.0159, 0.1755, 0.0151], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999998807907104\n",
      "Action selected index: tensor([16])\n",
      "Action selected prob : tensor([0.0709], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([6.])\n",
      "Reward for State tensor([[4.]]) Action tensor([6.]): tensor([-4.])\n",
      "New State: tensor([[10.]])\n",
      "Total reward accumulated: tensor([-313.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 6---------\n",
      "Action probabilities: tensor([1.8580e-03, 2.7759e-02, 1.0297e-03, 6.5952e-02, 6.2328e-02, 1.7723e-03,\n",
      "        3.5739e-03, 3.2186e-03, 1.2251e-03, 2.1066e-03, 4.6793e-01, 2.6087e-03,\n",
      "        3.9649e-05, 2.1828e-03, 2.3451e-05, 1.9189e-04, 5.0861e-02, 9.5549e-04,\n",
      "        4.8620e-04, 3.0362e-01, 2.7716e-04], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([19])\n",
      "Action selected prob : tensor([0.3036], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([9.])\n",
      "Reward for State tensor([[10.]]) Action tensor([9.]): tensor([-1.])\n",
      "New State: tensor([[-2.]])\n",
      "Total reward accumulated: tensor([-314.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 7---------\n",
      "Action probabilities: tensor([0.0791, 0.0302, 0.0143, 0.0241, 0.0586, 0.0485, 0.0366, 0.0227, 0.0667,\n",
      "        0.0519, 0.0574, 0.0423, 0.0733, 0.0158, 0.0613, 0.0521, 0.0397, 0.0635,\n",
      "        0.0539, 0.0582, 0.0501], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([0])\n",
      "Action selected prob : tensor([0.0791], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-10.])\n",
      "Reward for State tensor([[-2.]]) Action tensor([-10.]): tensor([-64.])\n",
      "New State: tensor([[9.]])\n",
      "Total reward accumulated: tensor([-378.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 8---------\n",
      "Action probabilities: tensor([2.9354e-03, 3.4688e-02, 1.6812e-03, 7.4639e-02, 7.0676e-02, 2.8548e-03,\n",
      "        5.5140e-03, 4.8793e-03, 2.1918e-03, 3.3163e-03, 4.3443e-01, 4.0526e-03,\n",
      "        9.3107e-05, 3.4647e-03, 5.8663e-05, 3.9319e-04, 5.7037e-02, 1.5572e-03,\n",
      "        9.2226e-04, 2.9404e-01, 5.7243e-04], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([19])\n",
      "Action selected prob : tensor([0.2940], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([9.])\n",
      "Reward for State tensor([[9.]]) Action tensor([9.]): tensor([-0.])\n",
      "New State: tensor([[-3.]])\n",
      "Total reward accumulated: tensor([-378.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 9---------\n",
      "Action probabilities: tensor([0.0921, 0.0235, 0.0081, 0.0184, 0.0593, 0.0475, 0.0237, 0.0151, 0.0679,\n",
      "        0.0485, 0.0587, 0.0313, 0.0861, 0.0084, 0.0640, 0.0578, 0.0456, 0.0842,\n",
      "        0.0456, 0.0725, 0.0415], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([19])\n",
      "Action selected prob : tensor([0.0725], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([9.])\n",
      "Reward for State tensor([[-3.]]) Action tensor([9.]): tensor([-144.])\n",
      "New State: tensor([[6.]])\n",
      "Total reward accumulated: tensor([-522.])\n",
      "---------------\n",
      "\n",
      "Starting State: tensor([[8.]])\n",
      "---------Time step: 0---------\n",
      "Action probabilities: tensor([4.5668e-03, 4.2685e-02, 2.7030e-03, 8.3183e-02, 7.8921e-02, 4.5285e-03,\n",
      "        8.3776e-03, 7.2841e-03, 3.8616e-03, 5.1410e-03, 3.9719e-01, 6.1997e-03,\n",
      "        2.1531e-04, 5.4157e-03, 1.4451e-04, 7.9339e-04, 6.2988e-02, 2.4993e-03,\n",
      "        1.7227e-03, 2.8042e-01, 1.1643e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([19])\n",
      "Action selected prob : tensor([0.2804], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([9.])\n",
      "Reward for State tensor([[8.]]) Action tensor([9.]): tensor([-1.])\n",
      "New State: tensor([[-4.]])\n",
      "Total reward accumulated: tensor([-1.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 1---------\n",
      "Action probabilities: tensor([0.1056, 0.0178, 0.0042, 0.0132, 0.0598, 0.0444, 0.0145, 0.0101, 0.0690,\n",
      "        0.0436, 0.0586, 0.0224, 0.0946, 0.0041, 0.0640, 0.0591, 0.0495, 0.1110,\n",
      "        0.0363, 0.0840, 0.0342], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([0])\n",
      "Action selected prob : tensor([0.1056], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-10.])\n",
      "Reward for State tensor([[-4.]]) Action tensor([-10.]): tensor([-36.])\n",
      "New State: tensor([[7.]])\n",
      "Total reward accumulated: tensor([-37.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 2---------\n",
      "Action probabilities: tensor([6.9624e-03, 5.1471e-02, 4.2586e-03, 9.0843e-02, 8.6358e-02, 7.0392e-03,\n",
      "        1.2473e-02, 1.0656e-02, 6.6668e-03, 7.8097e-03, 3.5584e-01, 9.2939e-03,\n",
      "        4.8790e-04, 8.2953e-03, 3.4884e-04, 1.5688e-03, 6.8162e-02, 3.9306e-03,\n",
      "        3.1534e-03, 2.6206e-01, 2.3205e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([3])\n",
      "Action selected prob : tensor([0.0908], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-7.])\n",
      "Reward for State tensor([[7.]]) Action tensor([-7.]): tensor([-196.])\n",
      "New State: tensor([[0.]])\n",
      "Total reward accumulated: tensor([-233.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 3---------\n",
      "Action probabilities: tensor([0.0464, 0.0502, 0.0246, 0.0423, 0.0583, 0.0415, 0.0645, 0.0382, 0.0796,\n",
      "        0.0408, 0.0597, 0.0509, 0.0429, 0.0359, 0.0409, 0.0405, 0.0315, 0.0308,\n",
      "        0.0608, 0.0533, 0.0666], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999998807907104\n",
      "Action selected index: tensor([8])\n",
      "Action selected prob : tensor([0.0796], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-2.])\n",
      "Reward for State tensor([[0.]]) Action tensor([-2.]): tensor([-4.])\n",
      "New State: tensor([[-2.]])\n",
      "Total reward accumulated: tensor([-237.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 4---------\n",
      "Action probabilities: tensor([0.0791, 0.0302, 0.0143, 0.0241, 0.0586, 0.0485, 0.0366, 0.0227, 0.0667,\n",
      "        0.0519, 0.0574, 0.0423, 0.0733, 0.0158, 0.0613, 0.0521, 0.0397, 0.0635,\n",
      "        0.0539, 0.0582, 0.0501], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([8])\n",
      "Action selected prob : tensor([0.0667], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-2.])\n",
      "Reward for State tensor([[-2.]]) Action tensor([-2.]): tensor([-0.])\n",
      "New State: tensor([[-4.]])\n",
      "Total reward accumulated: tensor([-237.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 5---------\n",
      "Action probabilities: tensor([0.1056, 0.0178, 0.0042, 0.0132, 0.0598, 0.0444, 0.0145, 0.0101, 0.0690,\n",
      "        0.0436, 0.0586, 0.0224, 0.0946, 0.0041, 0.0640, 0.0591, 0.0495, 0.1110,\n",
      "        0.0363, 0.0840, 0.0342], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0000001192092896\n",
      "Action selected index: tensor([1])\n",
      "Action selected prob : tensor([0.0178], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-9.])\n",
      "Reward for State tensor([[-4.]]) Action tensor([-9.]): tensor([-25.])\n",
      "New State: tensor([[8.]])\n",
      "Total reward accumulated: tensor([-262.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 6---------\n",
      "Action probabilities: tensor([4.5668e-03, 4.2685e-02, 2.7030e-03, 8.3183e-02, 7.8921e-02, 4.5285e-03,\n",
      "        8.3776e-03, 7.2841e-03, 3.8616e-03, 5.1410e-03, 3.9719e-01, 6.1997e-03,\n",
      "        2.1531e-04, 5.4157e-03, 1.4451e-04, 7.9339e-04, 6.2988e-02, 2.4993e-03,\n",
      "        1.7227e-03, 2.8042e-01, 1.1643e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([10])\n",
      "Action selected prob : tensor([0.3972], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([0.])\n",
      "Reward for State tensor([[8.]]) Action tensor([0.]): tensor([-64.])\n",
      "New State: tensor([[8.]])\n",
      "Total reward accumulated: tensor([-326.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 7---------\n",
      "Action probabilities: tensor([4.5668e-03, 4.2685e-02, 2.7030e-03, 8.3183e-02, 7.8921e-02, 4.5285e-03,\n",
      "        8.3776e-03, 7.2841e-03, 3.8616e-03, 5.1410e-03, 3.9719e-01, 6.1997e-03,\n",
      "        2.1531e-04, 5.4157e-03, 1.4451e-04, 7.9339e-04, 6.2988e-02, 2.4993e-03,\n",
      "        1.7227e-03, 2.8042e-01, 1.1643e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([10])\n",
      "Action selected prob : tensor([0.3972], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([0.])\n",
      "Reward for State tensor([[8.]]) Action tensor([0.]): tensor([-64.])\n",
      "New State: tensor([[8.]])\n",
      "Total reward accumulated: tensor([-390.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 8---------\n",
      "Action probabilities: tensor([4.5668e-03, 4.2685e-02, 2.7030e-03, 8.3183e-02, 7.8921e-02, 4.5285e-03,\n",
      "        8.3776e-03, 7.2841e-03, 3.8616e-03, 5.1410e-03, 3.9719e-01, 6.1997e-03,\n",
      "        2.1531e-04, 5.4157e-03, 1.4451e-04, 7.9339e-04, 6.2988e-02, 2.4993e-03,\n",
      "        1.7227e-03, 2.8042e-01, 1.1643e-03], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 0.9999999403953552\n",
      "Action selected index: tensor([4])\n",
      "Action selected prob : tensor([0.0789], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-6.])\n",
      "Reward for State tensor([[8.]]) Action tensor([-6.]): tensor([-196.])\n",
      "New State: tensor([[2.]])\n",
      "Total reward accumulated: tensor([-586.])\n",
      "---------------\n",
      "\n",
      "---------Time step: 9---------\n",
      "Action probabilities: tensor([0.0316, 0.0724, 0.0228, 0.0778, 0.0747, 0.0352, 0.0503, 0.0394, 0.0564,\n",
      "        0.0349, 0.1133, 0.0388, 0.0161, 0.0386, 0.0158, 0.0262, 0.0558, 0.0209,\n",
      "        0.0357, 0.1030, 0.0403], grad_fn=<SqueezeBackward1>)\n",
      "Sum of Action probabilities: 1.0\n",
      "Action selected index: tensor([3])\n",
      "Action selected prob : tensor([0.0778], grad_fn=<IndexBackward>)\n",
      "Action selected      : tensor([-7.])\n",
      "Reward for State tensor([[2.]]) Action tensor([-7.]): tensor([-81.])\n",
      "New State: tensor([[-5.]])\n",
      "Total reward accumulated: tensor([-667.])\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = create_trajectories(2, 10, policy, 1, space, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action_probs': [[tensor([0.0796], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0501], grad_fn=<IndexBackward>),\n",
       "   tensor([0.2804], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0363], grad_fn=<IndexBackward>),\n",
       "   tensor([0.2100], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0709], grad_fn=<IndexBackward>),\n",
       "   tensor([0.3036], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0791], grad_fn=<IndexBackward>),\n",
       "   tensor([0.2940], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0725], grad_fn=<IndexBackward>)],\n",
       "  [tensor([0.2804], grad_fn=<IndexBackward>),\n",
       "   tensor([0.1056], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0908], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0796], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0667], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0178], grad_fn=<IndexBackward>),\n",
       "   tensor([0.3972], grad_fn=<IndexBackward>),\n",
       "   tensor([0.3972], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0789], grad_fn=<IndexBackward>),\n",
       "   tensor([0.0778], grad_fn=<IndexBackward>)]],\n",
       " 'rewards': [[tensor([-4.]),\n",
       "   tensor([-144.]),\n",
       "   tensor([-1.]),\n",
       "   tensor([-144.]),\n",
       "   tensor([-16.]),\n",
       "   tensor([-4.]),\n",
       "   tensor([-1.]),\n",
       "   tensor([-64.]),\n",
       "   tensor([-0.]),\n",
       "   tensor([-144.])],\n",
       "  [tensor([-1.]),\n",
       "   tensor([-36.]),\n",
       "   tensor([-196.]),\n",
       "   tensor([-4.]),\n",
       "   tensor([-0.]),\n",
       "   tensor([-25.]),\n",
       "   tensor([-64.]),\n",
       "   tensor([-64.]),\n",
       "   tensor([-196.]),\n",
       "   tensor([-81.])]],\n",
       " 'states': [[tensor([[0.]]),\n",
       "   tensor([[-2.]]),\n",
       "   tensor([[8.]]),\n",
       "   tensor([[-4.]]),\n",
       "   tensor([[4.]]),\n",
       "   tensor([[4.]]),\n",
       "   tensor([[10.]]),\n",
       "   tensor([[-2.]]),\n",
       "   tensor([[9.]]),\n",
       "   tensor([[-3.]])],\n",
       "  [tensor([[8.]]),\n",
       "   tensor([[-4.]]),\n",
       "   tensor([[7.]]),\n",
       "   tensor([[0.]]),\n",
       "   tensor([[-2.]]),\n",
       "   tensor([[-4.]]),\n",
       "   tensor([[8.]]),\n",
       "   tensor([[8.]]),\n",
       "   tensor([[8.]]),\n",
       "   tensor([[2.]])]],\n",
       " 'actions': [[tensor([-2.]),\n",
       "   tensor([10.]),\n",
       "   tensor([9.]),\n",
       "   tensor([8.]),\n",
       "   tensor([0.]),\n",
       "   tensor([6.]),\n",
       "   tensor([9.]),\n",
       "   tensor([-10.]),\n",
       "   tensor([9.]),\n",
       "   tensor([9.])],\n",
       "  [tensor([9.]),\n",
       "   tensor([-10.]),\n",
       "   tensor([-7.]),\n",
       "   tensor([-2.]),\n",
       "   tensor([-2.]),\n",
       "   tensor([-9.]),\n",
       "   tensor([0.]),\n",
       "   tensor([0.]),\n",
       "   tensor([-6.]),\n",
       "   tensor([-7.])]],\n",
       " 'total_rewards': [tensor([-522.]), tensor([-667.])]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/FinalObjectDetection/venv_detectron/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_trajectory(space, traj_data, idx, color='orange', new_figure=True):\n",
    "    states = traj_data['states'][idx]\n",
    "    actions = traj_data['actions'][idx]\n",
    "\n",
    "    xes, yes = torch.meshgrid(space, space)\n",
    "\n",
    "    if new_figure:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.plot(xes.flatten(), yes.flatten(), 'p')\n",
    "\n",
    "    for i in range(1, len(states)):\n",
    "        prev_point = (states[i-1], actions[i-1])\n",
    "        curr_point = (states[i], actions[i])\n",
    "        grad = [curr_point[0]-prev_point[0], curr_point[1]-prev_point[1]]\n",
    "        grad = grad / np.sqrt(np.sum(np.power(grad, 2)))\n",
    "        scale_factor = 0.1\n",
    "        \n",
    "        plt.plot([prev_point[0], curr_point[0]], [prev_point[1], curr_point[1]], 'p-', color=color)\n",
    "        plt.arrow(0.5*(prev_point[0] + curr_point[0]), \n",
    "                  0.5*(prev_point[1] + curr_point[1]), \n",
    "                  scale_factor*grad[0],\n",
    "                  scale_factor*grad[1],\n",
    "                  width=0.1,          \n",
    "                  shape='full', \n",
    "                  length_includes_head=True,\n",
    "                  color='red')\n",
    "        \n",
    "    plt.title('Sample Trajectories')\n",
    "    plt.xlabel('States')\n",
    "    plt.ylabel(\"Actions\")\n",
    "\n",
    "    _ = plt.xticks(space)\n",
    "    _ = plt.yticks(space)\n",
    "    plt.grid()\n",
    "\n",
    "plot_trajectory(space, data, 0)\n",
    "plot_trajectory(space, data, 1, color='green', new_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: \n",
      " tensor([-5.])\n"
     ]
    }
   ],
   "source": [
    "def select_start_state(state_space, N=1):\n",
    "    '''Choose a start state uniformly randomly from state space\n",
    "    '''\n",
    "    state = state_space[torch.multinomial(torch.ones(N, len(state_space)), 1)]\n",
    "\n",
    "    if N==1:\n",
    "        state = state.squeeze(1)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Example: \\n\", select_start_state(space, N=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "T = 10\n",
    "\n",
    "mat_states = torch.zeros(N, T, 1)\n",
    "mat_actions = torch.zeros_like(mat_states)\n",
    "mat_action_probs = torch.zeros_like(mat_states)\n",
    "mat_rewards = torch.zeros_like(mat_states)\n",
    "\n",
    "mat_states[:,0] = select_start_state(space, N=N)\n",
    "\n",
    "#loop\n",
    "action_probs = policy(mat_states[:,0])\n",
    "action_selected_index = torch.multinomial(action_probs, 1)\n",
    "\n",
    "mat_action_probs[:,0] = action_probs.gather(1, action_selected_index)\n",
    "mat_actions[:,0] = space[action_selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_states[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_actions[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics(mat_states[:,0], mat_actions[:,0], 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mat_states[:,1] = dynamics(mat_states[:,0], mat_actions[:,0], 1, space)\n",
    "#compute reward at end given states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics(mat_states[:,0], mat_actions[:,0], 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_states[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_actions[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward(mat_states, mat_actions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_selected_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_action_probs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectories_optimized(N, T, policy, step_size, space, debug=False):\n",
    "    mat_states = torch.zeros(N, T, 1)\n",
    "    mat_actions = torch.zeros_like(mat_states)\n",
    "    mat_action_probs = torch.zeros_like(mat_states)\n",
    "    mat_rewards = torch.zeros_like(mat_states)\n",
    "\n",
    "    #initialize start state\n",
    "    mat_states[:,0] = select_start_state(space, N=N)\n",
    "\n",
    "    for t in range(T): #loop over time-steps\n",
    "        action_probs = policy(mat_states[:,t])\n",
    "        action_selected_index = torch.multinomial(action_probs, 1)\n",
    "\n",
    "        mat_action_probs[:,t] = action_probs.gather(1, action_selected_index)\n",
    "        mat_actions[:,t] = space[action_selected_index]\n",
    "\n",
    "        if t < T-1: #don't store state for actions taken at time-step T-1 (last time-step)\n",
    "            mat_states[:,t+1] = dynamics(mat_states[:,t], mat_actions[:,t], 1, space)\n",
    "    \n",
    "    #compute reward at end given states and actions\n",
    "    mat_rewards = reward(mat_states, mat_actions)\n",
    "    mat_total_rewards = mat_rewards.sum(dim=1)\n",
    "    \n",
    "    return {\n",
    "        'action_probs': mat_action_probs,\n",
    "        'rewards': mat_rewards,\n",
    "        'states': mat_states, \n",
    "        'actions': mat_actions,\n",
    "        'total_rewards': mat_total_rewards\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 ms, sys: 1.94 ms, total: 15.6 ms\n",
      "Wall time: 8.26 ms\n"
     ]
    }
   ],
   "source": [
    "%time data = create_trajectories_optimized(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 961 s, total: 189 ms\n",
      "Wall time: 189 ms\n"
     ]
    }
   ],
   "source": [
    "%time data = create_trajectories(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.38 ms  75.9 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data = create_trajectories_optimized(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 ms  9.34 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data = create_trajectories(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_optimized = create_trajectories_optimized(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(data['action_probs']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_optimized['actions'].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['action_probs'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Question**: For the optimized function, why is the result from timeit so much lower than from time? Some caching?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an (artificial) environment with a state space, dynamics, action space, reward function and even an untrained machine learning model for a policy.\n",
    "\n",
    "How do we train this policy? If this was a supervised problem, it would be easy. Take your data:\n",
    "\n",
    "(input, label) = $(\\vec{x}_i, y_i)$ for $i=1, \\ldots, N$\n",
    "\n",
    "Use your model, $p_{\\theta}(\\vec{x})$ to make predictions, compute a loss/cost that compares your prediction to the label and tune $\\theta$ to minimze the loss. To be more concrete, suppose we have a multi-class classification problem i.e. $y_i \\in \\{1, \\ldots, k\\}$ and the loss is cross-entropy:\n",
    "\n",
    "$$\\mathcal{L_i} = -\\Sigma_{j=1}^{k} y_{ij} \\log p_{ij}(\\vec{x_i})$$\n",
    "\n",
    "$y_{ij} = 1$ if the ith example has label $j$ and 0 otherwise (it's the one-hot encoded label vector). Similarly, $p_{ij}$ is the probability that the ith example has label $j$. By definition, $\\Sigma_{j=1}^{k} p_{ij} = 1$.\n",
    "\n",
    "To be clear, the sum over the classes always gives only one term. Since $y_{ij}=1$ for exactly one $j$ (the label), the loss reduces to:\n",
    "\n",
    "$$\\mathcal{L_i} = -y_{il} \\log p_{il}(\\vec{x_i}) = -\\log p_{il}(\\vec{x_i})$$\n",
    "\n",
    "where $l$ is the true label for the ith example. If the predicted probability is 1, i.e. the model predicted the correct label with full probability, then the loss is 0 ($\\log 1 = 0$). On the other end, theoretically, if the model predicted the probability of the true label to be 0, then the loss is $\\infty$ ($\\log 0 = -\\infty$). So we evaluate each example on the basic of the predicted probability of the true label. (This is also equivalent to maximum likelihood under a multinomial distribution).\n",
    "\n",
    "The total loss would just be the loss summed across all the training examples:\n",
    "\n",
    "$$\\mathcal{L} = \\Sigma_{i=1}^{n} \\mathcal{L}_i = -\\Sigma_{i=1}^n \\Sigma_{j=1}^{k} y_{ij}\\log p_{ij}(\\vec{x}_i)$$\n",
    "\n",
    "We can then update $\\theta$ by doing gradient descent as usual:\n",
    "\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_{\\theta} \\mathcal{L}$$\n",
    "\n",
    "If one is working with neural networks (which we will do exclusively in this notebook), the gradients can be computed using backpropagation. Auto-differentiation (\"autodiff\") libraries like PyTorch and TensorFlow make this easy for us. In pseudo-code, we would have something like:\n",
    "\n",
    "#given features and labels\n",
    "\n",
    "predictions = model(features)\n",
    "\n",
    "loss = loss_function(predictions, labels)\n",
    "\n",
    "loss.backward() #compute gradient\n",
    "\n",
    "update_parameters() #implements the gradient descent step\n",
    "\n",
    "But all this is possible because the thing we are minimizing (the loss) is \"directly connected to\" the parameters, $\\theta$. What this means is that the loss is a differentiable function of $\\theta$ through the predictions $p_\\theta$. This is not the case in reinforcement learning! The thing we want to maximize (the total reward, $R$) is not a differentiable function of the parameters of the policy, $\\theta$. The reward is something we receive from the environment. We have no way of even connecting the immediate reward at time-step t, $r_t$ to the policy except to know a loose correspondence that $r_t$ is somehow a consequence of all the steps taken till time t. So, how do we go about connecting changes in $R$ to changes in the policy, $\\theta$?\n",
    "\n",
    "It helps to stare at the quantity we are maximizing, the expected reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau} \\left(\\Sigma_{t=1}^{T} r_t\\right) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right) = \\mathbb{E}_{\\tau} R(\\tau) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "If it helps, think of the sample estimate from $n$ trajectories:\n",
    "\n",
    "$$J(\\theta) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} R(\\tau^{(i)}) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t^{(i)}|s_t^{(i)})\\right)$$\n",
    "\n",
    "I'll be pedantic here for the sake of clarity:\n",
    "\n",
    "$$J(\\theta) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} R(\\tau^{(i)}) \\left(\\log \\pi(a_1^{(i)}|s_1^{(i)}) + \\log \\pi(a_2^{(i)}|s_2^{(i)}) + \\ldots + \\log \\pi(a_T^{(i)}|s_T^{(i)})\\right)$$\n",
    "\n",
    "We can pass through the $R(\\tau^{(i)})$ term through the sum to get:\n",
    "\n",
    "$$J(\\theta) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} \\Sigma_{t=1}^{T} R(\\tau^{(i)})  \\log \\pi(a_t^{(i)}|s_t^{(i)})$$\n",
    "\n",
    "Compare this to the loss from the binary classification problem:\n",
    "\n",
    "$$\\mathcal{L} = \\Sigma_{i=1}^{n} \\mathcal{L}_i = -\\Sigma_{i=1}^n \\Sigma_{j=1}^{k} y_{ij}\\log p_{ij}(\\vec{x}_i) = -\\Sigma_{i=1}^n y_{il}\\log p_{il}(\\vec{x}_i) = -\\Sigma_{i=1}^n \\log p_{il}(\\vec{x}_i)$$\n",
    "\n",
    "where we collapse the second sum over the k classes by choosing the correct label class $l$ and also use $y_{il}=1$ by definition.\n",
    "\n",
    "The overall factor of $\\frac{1}{n}$ doesn't matter. Maximizing $J(\\theta)$ is equivalent to maximizing C$ J(\\theta)$ where C is a positive constant. Also, maximizing $J(\\theta)$ is equivalent to minimizing $-J(\\theta)$. So, we are minimizing:\n",
    "\n",
    "$$-J(\\theta) \\approx -\\Sigma_{i=1}^{n} \\Sigma_{t=1}^{T} R(\\tau^{(i)})  \\log \\pi(a_t^{(i)}|s_t^{(i)})$$\n",
    "\n",
    "What about the double-sum $\\Sigma_i\\Sigma_t$? Well, that's just saying that we are have $nT$ examples or data-points vs $n$ in the binary clasification problem? That's not crucial to our argument. So we are really just comparing the following two quantities:\n",
    "\n",
    "Binary classification:\n",
    "\n",
    "$$\\log p_{il}(y|\\vec{x}_i)$$ \n",
    "\n",
    "Policy Gradient:\n",
    "\n",
    "$$R(\\tau^{(i)})  \\log \\pi(a_t^{(i)}|s_t^{(i)})$$\n",
    "\n",
    "For binary classification, we make it explicit that we are predicting a probability distribution on $y_i$ (the k labels) given the features, $\\vec{x}_i$ just like for policy gradient, the policy/model predicts the probability distribution on the actions $a_t$ given the current state $s_t$.\n",
    "\n",
    "Let's summarize what we have learned till this point. For multi-class classification, we define the loss for an example to be the (negative) log of the probability that the model predicted for the correct label. There is a clear notion of what the correct label is. Maximizing this probability of predicting the label would lead to a lower loss.\n",
    "\n",
    "For policy gradients, we can treat each action in each trajectory as an independent data point. **We don't know what action is the \"correct\" action to take at any point in time.** So we declare that the action we took is the correct one! Mechanically this means that we now care about the (negative) log of the predicted probability of taking the action we actually performed. In the case of multiclass classification, we wanted to increase this probability by tuning the model's parameters. In the policy gradient case, we still aim to increase this probability but we weigh each log probability contribution by the total reward received for the trajectory that it was a part of. So increasing the probability of taking an action given a state by a fixed amount $\\epsilon$ will count for far more if the action was taken in a high-reward trajectory than if it was taken for a low reward trajectory.\n",
    "\n",
    "Furthermore, this equivalence between multi-class supervised learning and policy gradients has some convenient side-effects. We can simulate or run $n$ trajectories using our current policy (with possibly very sub-optimal parameters, $\\theta$), collect the list of probabilities, $\\pi(a_t|s_t)$, weight each one by the total reward of the trajectory (gotten after the end of the trajectory) and then define a loss that looks very similar to multi-class cross-entropy except that each $\\pi(a_t|s_t)$ term has a weight equal to the total reward in its trajectory. At this stage, we can use all the power of our auto-diff library to pretend we have a classification problem and optimize $J(\\theta)$.\n",
    "\n",
    "Weighing by $R(\\tau)$ is also intuitively satisfying: we should maximize the probability of actions taken during high-reward trajectories and minimize the probability of actions taken during low-reward trajectories.\n",
    "\n",
    "Let's see this fitting procedure in action. We are not focusing on code efficiency for now. Recall our starting point is the data from the trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_reward(s, debug=False):\n",
    "    #TODO: use torch tensor operations\n",
    "    action_probs = s['action_probs'] #action probabilities for each action at each time-step\n",
    "    total_rewards = s['total_rewards'] #total reward for each trajectory\n",
    "\n",
    "    total = 0\n",
    "    for idx, traj_reward in enumerate(total_rewards): #loop over every trajectory\n",
    "        traj_action_probs = s['action_probs'][idx] #get action probabilities at each time-step\n",
    "\n",
    "        if debug:\n",
    "            print(traj_reward)\n",
    "            print(traj_action_probs)\n",
    "\n",
    "        #naive computation of J\n",
    "        sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities for actions\n",
    "        total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs)\n",
    "\n",
    "    return total / len(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we are doing in the above function is computing:\n",
    "\n",
    "$$J \\approx \\frac{1}{n} \\Sigma_{i=1}^{n} R(\\tau^{(i)}) \\Sigma_{t=1}^T \\log \\pi(a_t^{(i)}|s_t^{(i)})$$\n",
    "\n",
    "We need to maximize this quantity or equivalently, minimize $-J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(optimizer, exp_reward, lr=1e-2):\n",
    "    optimizer.zero_grad()\n",
    "    (-exp_reward).backward() #doing gradient **ascent** - computes gradients\n",
    "    optimizer.step() #carries out gradient ascent update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put this all together in the following sequence:\n",
    "\n",
    "1. Generate $n$ trajectories\n",
    "\n",
    "2. Compute $J$ from these trajectories\n",
    "\n",
    "3. Do gradient ascent i.e. tune the policy's weights $\\theta$ to increase $J$\n",
    "\n",
    "4. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1, \n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr) #don't worry about this - variant of gradient descent\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter): #N_iterations\n",
    "    \n",
    "        #step 1: generate batch_size trajectories\n",
    "        s = create_trajectories(batch_size, T, policy, step_size, space, debug=debug)\n",
    "    \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward(s, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #Empirical Reward = mean of the total rewards for trajectories just generated\n",
    "            #Expected Reward = Mean(R(tau) * log P(tau)) which is NOT J but used to calculate gradients \n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "\n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 10000\n",
    "batch_size = 10\n",
    "T = 10\n",
    "\n",
    "policy, exp_reward_list = training_loop(N_iter, \n",
    "                                        batch_size, \n",
    "                                        T, \n",
    "                                        space, \n",
    "                                        policy=policy, \n",
    "                                        lr=1e-2, \n",
    "                                        step_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand what the above output signifies. The function runs N_iter iterations of the training loop. During each iteration, batch_size trajectories are generated using the current policy. Each trajectory has T time-steps. For each trajectory, at each time-step, the probability for taking the action that was taken is recorded. The total reward for each trajectory is also rewarded. Given these two quantities, we compute the total $J$ and then do back-propagation to calculate the gradients. These gradients are used to update the policy weights to **increase** $J$ (gradient ascent instead of descent).\n",
    "\n",
    "ADD NOTE ABOUT EXPECTED REWARD\n",
    "\n",
    "CLEAN UP\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. For each iteration, the average reward across batch_size trajectories is printed (Empirical Reward). The reward rapidly increases (becomes less negative) and ideally we would get 0 empirical reward i.e. at each time-step, the action picked was exactly equal to the state for our environment.\n",
    "\n",
    "2. For each iteration, we generate batch_size trajectories. Not only that, we never use the trajectories generated in previous iterations for the current iteration. In other words, we throw away all the data (which is generated sequentially on a CPU not the GPU). This is very different from supervised learning where we have a static dataset that stays constant. Is this justified? Can't we just keep a growing data-structure that has all the data generated till time t? The answer is no! Recall that when training supervised models, we make the assumption that the distribution that the training data is generated from is the same as the one the test data or new data that will be encountered is generated from. In the RL case, each time the policy changes, the distribution of actions taken and states visited is changed which means the training data at the current iteration is drawn from a different distribution from the training data in past iterations and they can be bundled together. This leads to the question of whether we can do some modifications or corrections to data from past iterations and will lead us to the notion of **importance sampling** (but later). \n",
    "\n",
    "Since the policy maps a state to an action and we know what the ideal action should be (ideal action = state for our environment), let's look at the policies outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, space[policy(space.unsqueeze(1)).argmax(dim=1)], 'p')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Action Chosen by Policy\")\n",
    "plt.title(\"Policy Max Probability Outputs\")\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see a strong correlation between the state and action which is great news! In the ideal case, the policy would realize that choosing the action to be equal to the state maximizes the reward. It comes close but has a hard-time \"resolving the reward structure at a finer level\". For example, the policy always picks the action 2 when the state varies from -1 to +4. It realizes that beyond this state, the action must be increased but can't figure out these it can increase the expected reward by choosing different actions within this range.\n",
    "\n",
    "We can already anticipate a potential problem. Suppose, our initial policy is very confident but wrong (unfortunately, a widespread phenomenon among humans today) about the actions it must take in every state. Then, as we generate different trajectories, we will never find out, for example, that taking the confident action +4 in state 0 is far worse than the action +1 in state 0. Since we will never try out action +1 in state 0, we will never learn and improve on what we know. This is the problem of **exploration** i.e. taking a risk and trying something different from what you (the policy) knows in the hope of finding a better action or outcome.\n",
    "\n",
    "The big question now is: How do we improve this?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go to improve the algorithm itself, let's optimize the code so we can do experiments faster. While it might be obvious that we spend most of our time in creating the trajectories, let's confirm this (only adding the decorator @profile below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile(immediate=True)\n",
    "def training_loop(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1, \n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr) #don't worry about this - variant of gradient descent\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter): #N_iterations\n",
    "    \n",
    "        #step 1: generate batch_size trajectories\n",
    "        s = create_trajectories(batch_size, T, policy, step_size, space, debug=debug)\n",
    "    \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward(s, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #Empirical Reward = mean of the total rewards for trajectories just generated\n",
    "            #Expected Reward = Mean(R(tau) * log P(tau)) which is NOT J but used to calculate gradients \n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "\n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO CHANGES HERE EXCEPT N_iter = 1000 (instead of 10,000)\n",
    "\n",
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 1000\n",
    "batch_size = 10\n",
    "T = 10\n",
    "\n",
    "policy, exp_reward_list = training_loop(N_iter, \n",
    "                                        batch_size, \n",
    "                                        T, \n",
    "                                        space, \n",
    "                                        policy=policy, \n",
    "                                        lr=1e-2, \n",
    "                                        step_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least on my machine, ~67% (20s/30s) of the total runtime is spent in create_trajectories. This is reasonable since we have a basic implementation consisting of double for-loops. Can we optimize this?\n",
    "\n",
    "Since all our trajectories have the same time-length (T), and since we can use the faster SIMD implementations of torch functions (to execute the policy and to generate random numbers), we should use matrices of size N (number of trajectories) x T (number of time-steps) to store the states, actions, probabilities of actions and rewards. The columns (each time-step) will be populated simultaneously.\n",
    "\n",
    "One optimized implementation is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectories_optimized(N, T, policy, step_size, space, debug=False):\n",
    "    mat_states = torch.zeros(N, T, 1)\n",
    "    mat_actions = torch.zeros_like(mat_states)\n",
    "    mat_action_probs = torch.zeros_like(mat_states)\n",
    "    mat_rewards = torch.zeros_like(mat_states)\n",
    "\n",
    "    #initialize start state\n",
    "    mat_states[:,0] = select_start_state(space, N=N)\n",
    "\n",
    "    for t in range(T): #loop over time-steps\n",
    "        action_probs = policy(mat_states[:,t].clone())\n",
    "        action_selected_index = torch.multinomial(action_probs, 1)\n",
    "\n",
    "        mat_action_probs[:,t] = action_probs.gather(1, action_selected_index)\n",
    "        mat_actions[:,t] = space[action_selected_index]\n",
    "\n",
    "        if t < T-1: #don't store state for actions taken at time-step T-1 (last time-step)\n",
    "            mat_states[:,t+1] = dynamics(mat_states[:,t], mat_actions[:,t], 1, space)\n",
    "    \n",
    "    #compute reward at end given states and actions\n",
    "    mat_rewards = reward(mat_states, mat_actions)\n",
    "    mat_total_rewards = mat_rewards.sum(dim=1)\n",
    "    \n",
    "    return {\n",
    "        'action_probs': mat_action_probs,\n",
    "        'rewards': mat_rewards,\n",
    "        'states': mat_states, \n",
    "        'actions': mat_actions,\n",
    "        'total_rewards': mat_total_rewards\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.49 ms, sys: 949 s, total: 6.44 ms\n",
      "Wall time: 3.48 ms\n"
     ]
    }
   ],
   "source": [
    "%time data = create_trajectories_optimized(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 179 ms, sys: 2.98 ms, total: 182 ms\n",
      "Wall time: 182 ms\n"
     ]
    }
   ],
   "source": [
    "%time data = create_trajectories(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24 ms  21.7 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data = create_trajectories_optimized(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 ms  864 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit data = create_trajectories(100, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future Question**: For the optimized function, why is the result from timeit so much lower than from time? Some caching?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: check if the functions result in the same trajectories if seeds are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "data_optimized = create_trajectories_optimized(1, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "data = create_trajectories(1, 10, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['states'])\n",
    "print(\"\\n\")\n",
    "print(data_optimized['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['actions'])\n",
    "print(\"\\n\")\n",
    "print(data_optimized['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['action_probs'])\n",
    "print(\"\\n\")\n",
    "print(data_optimized['action_probs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change training_loop so it just the optimized function. We will also need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_optimized['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(15)\n",
    "data = create_trajectories(3, 10, policy, 1, space)\n",
    "exp_reward = expected_reward(data, debug=True)\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_optimized['total_rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(15)\n",
    "data_optimized = create_trajectories_optimized(3, 10, policy, 1, space)\n",
    "exp_reward = expected_reward_optimized(data_optimized)\n",
    "print(exp_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_reward.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE create_trajectories -> create_trajectories_optimized\n",
    "#CHANGE expected_reward -> expected_reward_optimized\n",
    "\n",
    "def expected_reward(s, debug=False):\n",
    "    #TODO: use torch tensor operations\n",
    "    action_probs = s['action_probs'] #action probabilities for each action at each time-step\n",
    "    total_rewards = s['total_rewards'] #total reward for each trajectory\n",
    "\n",
    "    total = 0\n",
    "    for idx, traj_reward in enumerate(total_rewards): #loop over every trajectory\n",
    "        traj_action_probs = s['action_probs'][idx] #get action probabilities at each time-step\n",
    "\n",
    "        #if debug:\n",
    "        #    print(traj_reward)\n",
    "        #    print(traj_action_probs)\n",
    "\n",
    "        #naive computation of J\n",
    "        sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities for actions\n",
    "        if debug:\n",
    "            print(sum_log_prob)\n",
    "        total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs)\n",
    "\n",
    "    return total / len(total_rewards)\n",
    "\n",
    "def expected_reward_optimized(s, debug=False):\n",
    "    return (s['action_probs'].log().sum(dim=1) * s['total_rewards']).mean()\n",
    "\n",
    "@profile(immediate=True)\n",
    "def training_loop_optimized(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1, \n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr) #don't worry about this - variant of gradient descent\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter): #N_iterations\n",
    "    \n",
    "        #step 1: generate batch_size trajectories\n",
    "        s = create_trajectories_optimized(batch_size, T, policy, step_size, space, debug=debug)\n",
    "    \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward_optimized(s, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #Empirical Reward = mean of the total rewards for trajectories just generated\n",
    "            #Expected Reward = Mean(R(tau) * log P(tau)) which is NOT J but used to calculate gradients \n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"].numpy())}')\n",
    "\n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6288"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: Expected Reward = 158681.3125 Empirical Reward = -2128.60009765625\n",
      "Iter 1000: Expected Reward = 381.8773193359375 Empirical Reward = -54.470001220703125\n",
      "Iter 2000: Expected Reward = 115.95343017578125 Empirical Reward = -27.860000610351562\n",
      "Iter 3000: Expected Reward = 45.852664947509766 Empirical Reward = -22.3799991607666\n",
      "Iter 4000: Expected Reward = 24.895668029785156 Empirical Reward = -19.969999313354492\n",
      "Iter 5000: Expected Reward = 20.022628784179688 Empirical Reward = -18.90999984741211\n",
      "Iter 6000: Expected Reward = 5.7773637771606445 Empirical Reward = -18.8700008392334\n",
      "Iter 7000: Expected Reward = 3.719832420349121 Empirical Reward = -20.950000762939453\n",
      "Iter 8000: Expected Reward = 10.493110656738281 Empirical Reward = -18.81999969482422\n",
      "Iter 9000: Expected Reward = 3.765751838684082 Empirical Reward = -20.239999771118164\n",
      "\n",
      "*** PROFILER RESULTS ***\n",
      "training_loop_optimized (<ipython-input-49-abe4e38726f0>:28)\n",
      "function called 1 times\n",
      "\n",
      "         18520657 function calls (17320646 primitive calls) in 119.466 seconds\n",
      "\n",
      "   Ordered by: cumulative time, internal time, call count\n",
      "   List reduced from 101 to 40 due to restriction <40>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.095    0.095  119.466  119.466 <ipython-input-49-abe4e38726f0>:28(training_loop_optimized)\n",
      "    10000   20.063    0.002   78.422    0.008 <ipython-input-47-221d94879366>:1(create_trajectories_optimized)\n",
      "    10000    0.139    0.000   40.425    0.004 <ipython-input-27-87d9eaaa3021>:1(optimize)\n",
      "    10000    0.045    0.000   37.298    0.004 tensor.py:167(backward)\n",
      "    10000    0.063    0.000   37.253    0.004 __init__.py:44(backward)\n",
      "    10000   37.044    0.004   37.044    0.004 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "1500000/300000    4.208    0.000   36.360    0.000 module.py:522(__call__)\n",
      "   300000    3.136    0.000   34.911    0.000 <ipython-input-7-0075bac38070>:23(forward)\n",
      "   600000    1.157    0.000   15.834    0.000 linear.py:86(forward)\n",
      "   600000    0.927    0.000   14.317    0.000 functional.py:1355(linear)\n",
      "   600000   10.471    0.000   10.471    0.000 {built-in method addmm}\n",
      "   290000    9.963    0.000   10.087    0.000 <ipython-input-6-f07018bb5ad8>:13(dynamics)\n",
      "   300000    0.263    0.000    7.105    0.000 activation.py:1017(forward)\n",
      "   300000    0.175    0.000    6.841    0.000 functional.py:1202(softmax)\n",
      "   300000    6.666    0.000    6.666    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n",
      "   310000    4.447    0.000    4.447    0.000 {built-in method multinomial}\n",
      "   300000    4.324    0.000    4.324    0.000 {method 'gather' of 'torch._C._TensorBase' objects}\n",
      "   300000    0.234    0.000    3.042    0.000 activation.py:93(forward)\n",
      "    10000    0.892    0.000    2.821    0.000 adam.py:49(step)\n",
      "   300000    0.159    0.000    2.808    0.000 functional.py:904(relu)\n",
      "   600000    2.766    0.000    2.766    0.000 {method 't' of 'torch._C._TensorBase' objects}\n",
      "   300000    2.648    0.000    2.648    0.000 {built-in method relu}\n",
      "  2700000    1.625    0.000    1.625    0.000 module.py:562(__getattr__)\n",
      "   300000    1.612    0.000    1.612    0.000 {method 'clone' of 'torch._C._TensorBase' objects}\n",
      "    80000    0.916    0.000    0.916    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "  1500000    0.815    0.000    0.815    0.000 {built-in method torch._C._get_tracing_state}\n",
      "    10000    0.542    0.000    0.542    0.000 {built-in method zeros}\n",
      "    10000    0.110    0.000    0.518    0.000 <ipython-input-49-abe4e38726f0>:25(expected_reward_optimized)\n",
      "    10000    0.224    0.000    0.512    0.000 <ipython-input-13-9e8cc3767956>:1(select_start_state)\n",
      "  3300000    0.413    0.000    0.413    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "    80000    0.380    0.000    0.380    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "    10000    0.224    0.000    0.376    0.000 <ipython-input-6-f07018bb5ad8>:35(reward)\n",
      "   300000    0.215    0.000    0.337    0.000 container.py:167(__iter__)\n",
      "  1560001    0.257    0.000    0.284    0.000 {built-in method builtins.len}\n",
      "    20000    0.253    0.000    0.253    0.000 {method 'sum' of 'torch._C._TensorBase' objects}\n",
      "    40000    0.223    0.000    0.223    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}\n",
      "    40000    0.178    0.000    0.178    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}\n",
      "    40000    0.173    0.000    0.173    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}\n",
      "    10000    0.167    0.000    0.167    0.000 {method 'mean' of 'torch._C._TensorBase' objects}\n",
      "    10000    0.069    0.000    0.167    0.000 optimizer.py:159(zero_grad)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NO CHANGES HERE EXCEPT N_iter = 1000 (instead of 10,000)\n",
    "\n",
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 10000\n",
    "batch_size = 100\n",
    "T = 30\n",
    "\n",
    "policy, exp_reward_list = training_loop_optimized(N_iter, \n",
    "                                                  batch_size, \n",
    "                                                  T, \n",
    "                                                  space, \n",
    "                                                  policy=policy, \n",
    "                                                  lr=1e-2, \n",
    "                                                  step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAJcCAYAAABTzWhBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdfZxtdV33/9ebc0CQERRvJm8orby9KNQ5kXcZA3ibSimZlhbm1VFTL6w0NSz1ZxZpZaZdFHmTJjmaipSVCl5zJEMxDqIeAkUFkzuRQnGOKHj4/P7Y69g4zJwzwKy1v2fP6/l47MfZe6816/1ZM8zhfdZae+9UFZIkSRq/vcY9gCRJkkYsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJ60SSVyR5R3f/B5MsJNkw7rlakeTuSSrJxpv59ZXkR1dY9ktJPrzcukn+Msnv3rypJU0ai5m0h0lycZJru2L11SR/k2Tqpmyjqv6zqqaqascaznV4VzhOWfL8od3zW9Yqa9G2X5Hk+u578fUkZyZ58Frn3FJVdXJVPXKFZc+uqlfB976Hl9ySrCT3S/IPSb6R5JtJ5pM85CZ8/fcK/FpY6+1Jk85iJu2ZHl9VU8ADgU3Ay8Y8z05fAx6c5PaLnvsV4PM9Zr6r+17cEfgY8L4kWbrSzT0StidJ8iPAvwGfBe4B3AU4Bfhwi4VV0o1ZzKQ9WFVdCvwLcAhAkrt0R0v+O8kXkvzacl+39LRdkoOSvDXJZUmuTvL+7vltSR6/6Ov2TnJVkgesMNJ1wPuBp3TrbwB+ATh5Sf7rk3wlyTVJtib5qUXL/jnJnyx6PJfkLav4XlwPvA34AeD2SY5N8m9JXpfkv4BXJNkrycuSfDnJlUnenuTAJZv61e77cHmSFy6a47AkH++OzF2e5I1J9lnytY9N8qXue/TaJHt1X3tsko8tN3d3xPP3k+zP6Gd5l+4I4EL38/zW4qKb5IFJvpZk72U29wrg41V1fFX9d1V9s6r+HPhb4I+6r7/RUbnuKOxRSR4N/A7wC13+p7vlW5L8YZJPdj+zU5McdAu2d2z3ffpmkouS/NJy3xtpPbKYSXuwJAcDjwU+1T01B1zC6EjJMcAfJDliFZv6W+DWwP8C7gS8rnv+7cDTFq33WODyqvoUK3s78Mvd/UcB24DLlqzz78D9gYOAvwP+Psm+3bJfBZ6e5Ijuf9iHAcftbgeS3Ao4FvhKVV3VPf2TwJeAaeDV3fJjgVngh4Ep4I1LNjUL3BN4JPDiJEd1z+8AfgO4A/Bg4Ejg15d87c8xOoL5QODobl9Wpaq2A48BLutOM09V1WXAFuDJi1Z9OjDXFdGlHgH8/TLPvxt4aJL9djPDB4E/oDsKWVWHLlr8y93+3Bn4LvDnq9inG22vK6B/Djymqm4DPAQ4d3fbktYLi5m0Z3p/kq8zOnX3UUYF7GDgocCLq+rbVXUu8Cb+pyQtK8mdGRWCZ1fV1VV1fVV9tFv8DkZHgQ7oHj+dUYlbUVWdCRyU5N5d9tuXWecdVfVfVfXdqvoT4FbAvbtlVwDPYXT06/XAL1fVN3cR+eTue/EVYIZROdrpsqp6Q5dzLfBLwJ9W1ZeqagF4KfCUJac5X1lV26vqs8Bbgad2c22tqk9027oY+Cvgp5fM8kfdkar/BP5s59feQm+jK8fdEcinsvLP4A7A5cs8fzmjv+8PugVz/G1VbesK5O8y+r7f3BeP3AAckmS/qrq8qs67BXNJE8ViJu2ZfraqbltVP1RVv96VjrsA/72kxHwZuOtutnVw93VXL13QHbH5N+BJSW7LqMCdvHS9Zfwt8DxGR59OWbowyQuTnN9doP514EBGpWKnfwQ2AJ+rqmVPAS7y7u57caeqOqKqti5a9pUl696F0fdkpy8DGxkdUVvua77cfQ1J7pXkA0muSHINoyNBi2de8WtvoVOB+yW5B6MjYt+oqk+usO5VjI5oLXVnRmXoRj/jm2Dpvu3Njfd/t7pi9wvAs4HLk/xTkvvcgrmkiWIxkybHZYyOVN1m0XM/CFy6m6/7Svd1t11h+c4jNj/P6Pql3W0PRsXs14F/rqpvLV7QXU/224xOz92uqm4LfANYfMH+q4HzgTsnuSVHnWrJ48uAH1r0+AcZnZb76qLnDl6yfOdp2BOBC4B7VtUBjK6dWvoig5W+9ubOS1V9m9GpyKex+yOWpzP6OS31ZEY/u28B2xmdtga+dxTujruaobN0365nVARv8vaq6kNV9QhGhfEC4K9X3iVpfbGYSROiqr4CnAn8YZJ9k/w48ExGpyN39XWXM7ro/P8muV13gf/DF63yfkbXTB3HMqclV9jmRYxO8x2/zOLbMCpDXwM2Jvk9YOepUrrsZzA6DforwBuS7O6o32q9E/iNJPfI6C1Gdl7/9N1F6/xuklsn+V/dHO9aNPc1wEJ3hOc5y2z/Rd338GBG3693LbPOrnyV0QsXlr4g4e2Mro17ArsuZq8EHpLk1Rm9oOM2SZ7P6Hv54m6dzwP7JvmZ7gUEL2N0KnnxDHff+cKFRZ6W0Vtx3Br4/4D3dG+3cpO2l2Q6ydHdtWbfARYYHc2ThMVMmjRPBe7O6EjNKcDLq+r0VXzd0xkdAbkAuBJ4wc4F3WnS9zJ6+4X3rXaQqvpYdyp0qQ8BH2T0P/QvA9+mO03WXcv2duB5VXVpVf0r8GbgrcmN3wLjZngLo2JzBnBRl/38Jet8FPgC8BHgj6tq5xvDvhD4ReCbjI7wLFe6TgW2MrqY/Z+62Vetqi5gVB6/1L368y7d8//GqLycU1Vf3sXXXwg8DDgUuJjRtWVPAh7VbYOq+gajo5lvYnQ0dTujF4zstPPFA/+V5JxFz/8t8DfAFcC+wP+5mdvbC/hNRv+N/jejAr9cyZXWpVStdNRakka6o1r3qqqn7XZl9SLJ/wP+rqreNIbsLcA7xpEtrTcT/4aLkm6Z7v2qnsnoqJrGIMlP8D9vwSFpgo31VGaSt2T0Jo/bFj13UJLTklzY/Xm7cc4orWcZvUHtV4B/qaozxj3PepTkbYwu6n/Bbt42RNIEGOupzO4i3wXg7VW1853LX8PopfsnJHkJo1dtvXhX25EkSZoEY7/GLMndgQ8sKmafAw6vqsu7N77cUlX3HuOIkiRJg2jxGrPp7uX7MHr1z/RyKyXZDGwG2G+//WYOPvjg5VZbUzfccAN77dXv2d9JyRgqZ1IyhsqZlIyhctyX9jKGypmUjKFyJiVjqJzPf/7zV1XVHZddWFVjvTF6af+2RY+/vmT51bvbxszMTA1hfn7ejMZyJiVjqJxJyRgqx31pL2OonEnJGCpnUjKGygHOrhU6TYvvY/bV7hTmzs/wu3LM80iSJA2ixWL2D4ze7Zvuz1PHOIskSdJgxv12Ge8EPg7cO8klSZ4JnAA8IsmFwFHdY0mSpIk31ov/q2qlDyc+ctBBJEmSGtDiqUxJkqR1yWImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1Igmi1mS45JsS3JekheMex5JkqQhNFfMkhwC/BpwGHAo8LgkPzreqSRJkvrXXDED7gucVVXfqqrvAh8FnjjmmSRJknqXqhr3DN8nyX2BU4EHA9cCHwHOrqrnL1lvM7AZYHp6emZubq732RYWFpiamjKjoZxJyRgqZ1IyhspxX9rLGCpnUjKGypmUjKFyZmdnt1bVpmUXVlVzN+CZwFbgDOBE4M92tf7MzEwNYX5+3ozGciYlY6icSckYKsd9aS9jqJxJyRgqZ1IyhsphdMBp2U7T4qlMqurNVTVTVQ8HrgY+P+6ZJEmS+rZx3AMsJ8mdqurKJD/I6PqyB417JkmSpL41WcyA9ya5PXA98Nyq+vq4B5IkSepbk8Wsqn5q3DNIkiQNrclrzCRJktYji5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjWi2mCX5jSTnJdmW5J1J9h33TJIkSX1qspgluSvwf4BNVXUIsAF4yninkiRJ6leTxayzEdgvyUbg1sBlY55HkiSpV6mqcc+wrCTHAa8GrgU+XFW/tGT5ZmAzwPT09Mzc3FzvMy0sLDA1NWVGQzmTkjFUzqRkDJXjvrSXMVTOpGQMlTMpGUPlzM7Obq2qTcsurKrmbsDtgP8H3BHYG3g/8LSV1p+ZmakhzM/Pm9FYzqRkDJUzKRlD5bgv7WUMlTMpGUPlTErGUDnA2bVCp2n1VOZRwEVV9bWquh54H/CQMc8kSZLUq1aL2X8CD0py6yQBjgTOH/NMkiRJvWqymFXVWcB7gHOAzzKa86SxDiVJktSzjeMeYCVV9XLg5eOeQ5IkaShNHjGTJElajyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY1ospgluXeScxfdrknygnHPJUnSnmLHDcVJZ3yR535kOyed8UV23FBmNJCzOxvHkrobVfU54P4ASTYAlwKnjHUoSZL2EBddtZ3nnnwOF121nWuvh9eddiGnnnsZb/zFB3KPO+xvxphyVqPJI2ZLHAl8saq+PO5BJEnaExxz4plccMU1XHv9DgCuvX4H519+DceceKYZY8xZjVSN51DdaiV5C3BOVb1xyfObgc0A09PTM3Nzc73PsrCwwNTUlBkN5UxKxlA5k5IxVI770l7GUDl7esYJZ13LBVffcKPn73PQXrzksP3MGFPOTrOzs1uratNyy5o8lblTkn2AJwAvXbqsqk4CTgLYtGlTHX744b3Ps2XLFvrOmZSMoXImJWOonEnJGCrHfWkvY6icPT3j6gMv4WWnbGP7dTu+99z++2zgWY84hMMfcDczxpSzGq2fynwMo6NlXx33IJIk7SmOvO80G/bK9z23Ya9w5H2nzRhjzmo0fcQMeCrwznEPIUnSnuSAfffmM694FNDfkblJyRgyZzWaPWKWZH/gEcD7xj2LJEnSEJo9YlZV24Hbj3sOSZKkoTR7xEySJGm9sZhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNaLZYpbktknek+SCJOcnefC4Z5IkSerTxnEPsAuvBz5YVcck2Qe49bgHkiRJ6lOTxSzJgcDDgWMBquo64LpxziRJktS3VNW4Z7iRJPcHTgL+AzgU2AocV1XbF62zGdgMMD09PTM3N9f7XAsLC0xNTZnRUM6kZAyVMykZQ+W4L+1lDJUzKRlD5UxKxlA5s7OzW6tq07ILq6q5G7AJ+C7wk93j1wOvWmn9mZmZGsL8/LwZjeVMSsZQOZOSMVSO+9JexlA5k5IxVM6kZAyVA5xdK3SaVi/+vwS4pKrO6h6/B3jgGOeRJEnqXZPFrKquAL6S5N7dU0cyOq0pSZI0sZq8+L/zfODk7hWZXwKeMeZ5JEmSetVsMauqcxldayZJkrQuNHkqU5IkaT2ymEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1YuO4B1hJkouBbwI7gO9W1abxTiRJktSvZotZZ7aqrhr3EJIkSUPwVKYkSVIjUlXjnmFZSS4CrgYK+KuqOmnJ8s3AZoDp6emZubm53mdaWFhgamrKjIZyJiVjqJxJyRgqx31pL2OonEnJGCpnUjKGypmdnd264iVaVdXkDbhr9+edgE8DD19p3ZmZmRrC/Py8GY3lTErGUDmTkjFUjvvSXsZQOZOSMVTOpGQMlQOcXSt0mmZPZVbVpd2fVwKnAIeNdyJJkqR+NVnMkuyf5DY77wOPBLaNdypJkqR+tfqqzGnglCQwmvHvquqD4x1JkiSpX00Ws6r6EnDouOeQJEkaUpOnMiVJktYji5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSI5otZkk2JPlUkg+MexZJ6sOOG4qTzvgiz/3Idk4644vsuKHMGHPOpGRoz9VsMQOOA84f9xCS1IeLrtrO49/wMV532oVsvx5ed9qFPOGNH+Oiq7abMaacScnQnq3JYpbkbsDPAG8a9yyS1IdjTjyTC664hmuv3wHAtdfv4PzLr+GYE880Y0w5k5KhPVuq2juEmuQ9wB8CtwFeWFWPW2adzcBmgOnp6Zm5ubne51pYWGBqasqMhnImJWOonEnJGCqnz4wTzrqWC66+4UbP3+egvXjJYfuZMYacSclYak//XRkyY6ic2dnZrVW1abllG3tNvhmSPA64sqq2Jjl8pfWq6iTgJIBNmzbV4YevuOqa2bJlC33nTErGUDmTkjFUzqRkDJXTZ8bVB17Cy07Zxvbrdnzvuf332cCzHnEIhz/gbmaMIWdSMpba039XhswYMmclLZ7KfCjwhCQXA3PAEUneMd6RJGltHXnfaTbsle97bsNe4cj7TpsxppxJydCerbkjZlX1UuClAN0RsxdW1dPGOpQkrbED9t2bz7ziUUB//0KflIyhciYlQ3u2Fo+YSZIkrUvNHTFbrKq2AFvGPIYkSdIgPGImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiOaLGZJ9k3yySSfTnJekleOeyZJkqS+bRz3ACv4DnBEVS0k2Rv4WJJ/qapPjHswSZKkvjRZzKqqgIXu4d7drcY3kSRJUv8y6kDtSbIB2Ar8KPAXVfXiJcs3A5sBpqenZ+bm5nqfaWFhgampKTMaypmUjKFyJiVjqBz3pb2MoXImJWOonEnJGCpndnZ2a1VtWnZhVTV9A24LzAOHrLTOzMxMDWF+ft6MxnImJWOonEnJGCrHfWkvY6icSckYKmdSMobKAc6uFTpNkxf/L1ZVX2dUzB497lkkSZL61GQxS3LHJLft7u8HPAK4YLxTSZIk9avJi/+BOwNv664z2wt4d1V9YMwzSZIk9arJYlZVnwEeMO45JEmShtTkqUxJkqT1yGImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiN2W8ySvC/JzyQZrMQlOTjJfJL/SHJekuOGypYkSRqX1ZSt/wv8InBhkhOS3LvnmQC+C/xWVd0PeBDw3CT3GyBXkiRpbHZbzKrq9Kr6JeCBwMXA6UnOTPKMJHv3MVRVXV5V53T3vwmcD9y1jyxJkqRWpKp2v1Jye+BpwNOBy4CTgYcBP1ZVh/c6YHJ34AzgkKq6ZtHzm4HNANPT0zNzc3N9jgHAwsICU1NTZjSUMykZQ+VMSsZQOe5LexlD5UxKxlA5k5IxVM7s7OzWqtq07MKq2uUNOAX4D+ClwJ2XLDt7d19/S27AFLAVeOKu1puZmakhzM/Pm9FYzqRkDJUzKRlD5bgv7WUMlTMpGUPlTErGUDm76k8bV1Hs/ryq5lcodcu3vTXQnSZ9L3ByVb2vrxxJkqRWrObi//slue3OB0lul+TXe5yJJAHeDJxfVX/aZ5YkSVIrVlPMfq2qvr7zQVVdDfxafyMB8FBG17MdkeTc7vbYnjMlSZLGajWnMjckSXdOlCQbgH36HKqqPgakzwxJkqTWrKaYfRB4V5K/6h4/q3tOkiRJa2g1xezFjMrYc7rHpwFv6m0iSZKkdWq3xayqbgBO7G6SJEnqyYrFLMm7q+rJST4L3OhdaKvqx3udTJIkaZ3Z1RGznR8c/rghBpEkSVrvVixmVXV59+eXhxtHkiRp/drVqcxv8v2nMNM9DlBVdUDPs0mSJK0ruzpidpshB5EkSVrvVvN2GSQ5FPip7uEZVfWZ/kaSJElan3b7kUxJjgNOBu7U3U5O8vy+B5MkSVpvVnPE7JnAT1bVdoAkfwR8HHhDn4NJkiStN6v5EPMAOxY93oGfYylJkrTmVnPE7K3AWUlO6R7/LPDm/kaSJElan1bzkUx/mmQL8LDuqWdU1ad6nUqSJGkd2tX7mO0LPBv4UeCzwP+tqu8ONZgkSdJ6s6trzN4GbGJUyh4D/PEgE0mSJK1TuzqVeb+q+jGAJG8GPjnMSJIkSevTro6YXb/zjqcwJUmS+rerI2aHJrmmux9gv+5x75+VmeQtwOOAK6vqkL5yJEmSWrLiEbOq2lBVB3S321TVxkX3+/4A878BHt1zhiRJUlNW8wazg6uqM4D/HvcckiRJQ0pVjXuGZSW5O/CBlU5lJtkMbAaYnp6emZub632mhYUFpqamzGgoZ1IyhsqZlIyhctyX9jKGypmUjKFyJiVjqJzZ2dmtVbVp2YVV1eQNuDuwbTXrzszM1BDm5+fNaCxnUjKGypmUjKFy3Jf2MobKmZSMoXImJWOoHODsWqHT7PZUZpLnJ7ndWjZFSZIk3dhqrjGbBv49ybuTPDqJH2AuSZLUg90Ws6p6GXBPRh9cfixwYZI/SPIjfQ2V5J3Ax4F7J7kkyTP7ypIkSWrFbj/EHEZvWpbkCuAK4LvA7YD3JDmtqn57rYeqqqeu9TYlSZJat9tiluQ44JeBq4A3AS+qquuT7AVcCKx5MZMkSVqPVnPE7CDgiVX15cVPVtUNSR7Xz1iSJEnrz26LWVW9PMkDkxwNFPBvVXVOt+z8vgeUJElaL1bzdhm/C7wNuD1wB+CtSV7W92CSJEnrzWpOZT4NOLSqvg2Q5ATgXOD3+xxMkiRpvVnN+5hdBuy76PGtgEv7GUeSJGn9WvGIWZI3MLqm7BvAeUlO6x4/AvjkMONJkiStH7s6lXl29+dW4JRFz2/pbRpJkqR1bMViVlVvG3IQSZKk9W4115hJkiRpABYzSZKkRljMJEmSGrGaz8q8F/Ai4IcWr19VR/Q4lyRJ0rqzmjeY/XvgL4G/Bnb0O44kSdL6tZpi9t2qOrH3SSRJkta51Vxj9o9Jfj3JnZMctPPW+2SSJEnrzGqOmP1K9+eLFj1XwA+v/TiSJEnr126LWVXdY4hBJEmS1rvdnspMcuskL0tyUvf4nkke1/dgSR6d5HNJvpDkJX3nSZIkjdtqrjF7K3Ad8JDu8aXA7/c2EZBkA/AXwGOA+wFPTXK/PjMlSZLGbTXF7Eeq6jXA9QBV9S0gvU4FhwFfqKovVdV1wBxwdM+ZkiRJY5Wq2vUKyZnAkcC/VdUDk/wI8M6qOqy3oZJjgEdX1f/uHj8d+Mmqet6idTYDmwGmp6dn5ubm+hrnexYWFpiamjKjoZxJyRgqZ1IyhspxX9rLGCpnUjKGypmUjKFyZmdnt1bVpmUXVtUub8AjgI8CXwNOBi4GDt/d192SG3AM8KZFj58OvHGl9WdmZmoI8/PzZjSWMykZQ+VMSsZQOe5LexlD5UxKxlA5k5IxVA5wdq3QaVbzqszTkpwDPIjRKczjquqqW1gWd+dS4OBFj+/WPSdJkjSxVvOqzIcC366qfwJuC/xOkh/qea5/B+6Z5B5J9gGeAvxDz5mSJEljtZqL/08EvpXkUOA3gS8Cb+9zqKr6LvA84EPA+cC7q+q8PjMlSZLGbbWflVlJjgb+oqrenOSZfQ9WVf8M/HPfOZIkSa1YTTH7ZpKXMroA/6eS7AXs3e9YkiRJ689qTmX+AvAd4Fer6gpGF+K/ttepJEmS1qHdFrOujJ0MHNh9FNO3q6rXa8wkSZLWo9W8KvPJwCeBnweeDJzVvQGsJEmS1tBqrjE7HviJqroSIMkdgdOB9/Q5mCRJ0nqzmmvM9tpZyjr/tcqvkyRJ0k2wmiNmH0zyIeCd3eNfwLexkCRJWnOr+UimFyV5EvDQ7qmTquqUfseSJElaf1ZzxIyqei/w3p5nkSRJWtdW86rMJya5MMk3klyT5JtJrhliOEmSpPVkNUfMXgM8vqrO73sYSZKk9Ww1r678qqVMkiSpfyseMUvyxO7u2UneBbyf0UczAVBV7+t5NkmSpHVlV6cyH7/o/reARy56XIDFTJIkaQ2tWMyq6hlDDiJJkrTerXiNWZLXJnnWMs8/K8kJ/Y4lSZK0/uzq4v8jgJOWef6vgcf1M44kSdL6tatidquqqqVPVtUNQPobSZIkaX3aVTG7Nsk9lz7ZPXdtXwMl+fkk5yW5IcmmvnIkSZJas6ti9nvAvyQ5NsmPdbdnAP/ULevLNuCJwBk9ZkiSJDVnV6/K/JckPwu8CHh+9/Q24ElV9dm+Btr5ZraJZ0slSdL6kmUuI2tCki3AC6vq7BWWbwY2A0xPT8/Mzc31PtPCwgJTU1NmNJQzKRlD5UxKxlA57kt7GUPlTErGUDmTkjFUzuzs7NaqWv5yraoa/Aaczujo29Lb0YvW2QJsWs32ZmZmagjz8/NmNJYzKRlD5UxKxlA57kt7GUPlTErGUDmTkjFUDnB2rdBpVvMh5muuqo4aR64kSVLLVvMh5pIkSRrAbo+YJbkj8GvA3RevX1W/2sdASX4OeANwR+CfkpxbVY/qI0uSJKklqxpU+7cAABtZSURBVDmVeSrwr4yuC9vR7zhQVacAp/SdI0mS1JrVFLNbV9WLe59EkiRpnVvNNWYfSPLY3ieRJEla51ZTzI5jVM6+neSb3e2avgeTJElab3Z7KrOqbjPEIJIkSevdqt7HLMkTgId3D7dU1Qf6G0mSJGl92u2pzCQnMDqd+R/d7bgkf9j3YJIkSevNao6YPRa4f1XdAJDkbcCngJf2OZgkSdJ6s9p3/r/tovsH9jGIJEnSereaI2Z/CHwqyTwQRteavaTXqSRJktah1bwq851JtgA/0T314qq6otepJEmS1qEVT2UmuU/35wOBOwOXdLe7dM9JkiRpDe3qiNlvApuBP1lmWQFH9DKRJEnSOrViMauqzd3dx1TVtxcvS7Jvr1NJkiStQ6t5VeaZq3xOkiRJt8CKR8yS/ABwV2C/JA9g9IpMgAOAWw8wmyRJ0rqyq2vMHgUcC9yN0XVmO4vZNcDv9DuWJEnS+rOra8zeBrwtyZOq6r0DziRJkrQureYas5kk33vn/yS3S/L7Pc4kSZK0Lq2mmD2mqr6+80FVXc3o8zN7keS1SS5I8pkkpywuhZIkSZNsNcVsQ5Jb7XyQZD/gVrtY/5Y6DTikqn4c+Dx+WLokSVonVvNZmScDH0ny1u7xM4C39zVQVX140cNPAMf0lSVJktSSVNXuV0oeDRzVPTytqj7U61T/k/uPwLuq6h3LLNvM6JMJmJ6enpmbm+t9noWFBaampsxoKGdSMobKmZSMoXLcl/YyhsqZlIyhciYlY6ic2dnZrVW1admFVXWTbsDDgL+4qV+3ZBunA9uWuR29aJ3jgVPoyuOubjMzMzWE+fl5MxrLmZSMoXImJWOoHPelvYyhciYlY6icSckYKgc4u1boNKs5lUn3BrNPBZ4MXAS872bXxFEZPGpXy5McCzwOOLLbAUmSpIm3q3f+vxejMvZU4CrgXYyOXs32OVB32vS3gZ+uqm/1mSVJktSSXR0xuwD4V+BxVfUFgCS/McBMb2T0qs/TkgB8oqqePUCuJEnSWO2qmD0ReAown+SDwBz/87FMvamqH+07Q5IkqUUrvo9ZVb2/qp4C3AeYB14A3CnJiUkeOdSAkiRJ68Vu32C2qrZX1d9V1eMZfaD5p4AX9z6ZJEnSOrOad/7/nqq6uqpOqqoj+xpIkiRpvbpJxUySJEn9sZhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSI5orZkleleQzSc5N8uEkdxn3TFqfdtxQnHTGF3nuR7Zz0hlfZMcNtcfmTErGUDlD7YskLdVcMQNeW1U/XlX3Bz4A/N64B9L6c9FV23n8Gz7G6067kO3Xw+tOu5AnvPFjXHTV9j0uZ1IyhsoZal8kaTnNFbOqumbRw/0B/6mqwR1z4plccMU1XHv9DgCuvX4H519+DceceOYelzMpGUPlDLUvkrScVLXXe5K8Gvhl4BvAbFV9bZl1NgObAaanp2fm5uZ6n2thYYGpqSkzGsrpK+OEs67lgqtvuNHz9zloL15y2H57VM6kZAyVM9S+LLYn/64MnTFUzqRkDJUzKRlD5czOzm6tqk3LLdvYa/IKkpwO/MAyi46vqlOr6njg+CQvBZ4HvHzpilV1EnASwKZNm+rwww/vceKRLVu20HfOpGQMldNXxtUHXsLLTtnG9ut2fO+5/ffZwLMecQiHP+Bue1TOpGQMlTPUviy2J/+uDJ0xVM6kZAyVMykZQ+asZCynMqvqqKo6ZJnbqUtWPRl40jhm1Pp25H2n2bBXvu+5DXuFI+87vcflTErGUDlD7YskLWcsR8x2Jck9q+rC7uHRwAXjnEfr0wH77s1nXvEooN9/PQ2RMykZQ+UMtS+StJzmihlwQpJ7AzcAXwaePeZ5JEmSBtFcMasqT11KkqR1qbm3y5AkSVqvLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjWi2mCX5rSSV5A7jnkWSJGkITRazJAcDjwT+c9yzSJIkDaXJYga8DvhtoMY9iCRJ0lBS1Vb3SXI0cERVHZfkYmBTVV21zHqbgc0A09PTM3Nzc73PtrCwwNTUlBkN5UxKxlA5k5IxVI770l7GUDmTkjFUzqRkDJUzOzu7tao2Lbuwqga/AacD25a5HQ2cBRzYrXcxcIfdbW9mZqaGMD8/b0ZjOZOSMVTOpGQMleO+tJcxVM6kZAyVMykZQ+UAZ9cKnWZjr5VwBVV11HLPJ/kx4B7Ap5MA3A04J8lhVXXFgCNKkiQNbizFbCVV9VngTjsf7+pUpiRJ0qRp9eJ/SZKkdaepI2ZLVdXdxz2DJEnSUDxiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjmitmSV6R5NIk53a3x457JkmSpCFsHPcAK3hdVf3xuIeQJEkaUnNHzCRJktarVNW4Z/g+SV4BHAtcA5wN/FZVXb3MepuBzQDT09Mzc3Nzvc+2sLDA1NSUGQ3lTErGUDmTkjFUjvvSXsZQOZOSMVTOpGQMlTM7O7u1qjYtu7CqBr8BpwPblrkdDUwDGxgdzXs18JbdbW9mZqaGMD8/b0ZjOZOSMVTOpGQMleO+tJcxVM6kZAyVMykZQ+UAZ9cKnWYs15hV1VGrWS/JXwMf6HkcSZKkJjR3jVmSOy96+HOMjqRJkiRNvBZflfmaJPcHCrgYeNZ4x5EkSRpGc8Wsqp4+7hkkSZLGoblTmZIkSeuVxUySJKkRFjNJkqRGWMwkSZIaYTGTJElqhMVMkiSpERYzSZKkRljMJEmSGmExkyRJaoTFTJIkqREWM0mSpEZYzCRJkhphMZMkSWqExUySJKkRFjNJkqRGWMwkSZIaYTGTJElqhMVMkiSpEU0WsyTPT3JBkvOSvGbc80iSJA1h47gHWCrJLHA0cGhVfSfJncY9kyRJ0hBaPGL2HOCEqvoOQFVdOeZ5JEmSBpGqGvcM3yfJucCpwKOBbwMvrKp/X2a9zcBmgOnp6Zm5ubneZ1tYWGBqasqMhnImJWOonEnJGCrHfWkvY6icSckYKmdSMobKmZ2d3VpVm5ZdWFWD34DTgW3L3I7u/nwDEOAw4CK6ArnSbWZmpoYwPz9vRmM5k5IxVM6kZAyV4760lzFUzqRkDJUzKRlD5QBn1wqdZizXmFXVUSstS/Ic4H3d4J9McgNwB+BrQ80nSZI0Di1eY/Z+YBYgyb2AfYCrxjqRJEnSAJp7VSbwFuAtSbYB1wG/0h09kyRJmmjNFbOqug542rjnkCRJGlqLpzIlSZLWJYuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiOaK2ZJ3pXk3O52cZJzxz3TjhuKk874Is/9yHZOOuOL7LihzBhzzlD7IknSkDaOe4ClquoXdt5P8ifAN8Y4DhddtZ3nnnwOF121nWuvh9eddiGnnnsZb/zFB3KPO+xvxhhyhtoXSZKG1twRs52SBHgy8M5xznHMiWdywRXXcO31OwC49vodnH/5NRxz4plmjClnqH2RJGloqWrzFFCShwN/WlWbVli+GdgMMD09PTM3N9fLHCecdS0XXH3DjZ6/z0F78ZLD9jNjDDlD7ctOCwsLTE1Nrfl2x5EzKRlD5bgv7WUMlTMpGUPlTErGUDmzs7NbV+o3YzmVmeR04AeWWXR8VZ3a3X8quzhaVlUnAScBbNq0qQ4//PC1HhOAqw+8hJedso3t1+343nP777OBZz3iEA5/wN3MGEPOUPuy05YtW+jrv6+hcyYlY6gc96W9jKFyJiVjqJxJyRgyZyVjOZVZVUdV1SHL3E4FSLIReCLwrnHMt9iR951mw175vuc27BWOvO+0GWPKGWpfJEkaWnMX/3eOAi6oqkvGPcgB++7NZ17xKKC/Fj0pGUPlDLUvkiQNrdWL/5/CmC/6lyRJGlqTR8yq6thxzyBJkjS0Vo+YSZIkrTsWM0mSpEZYzCRJkhphMZMkSWqExUySJKkRFjNJkqRGWMwkSZIaYTGTJElqhMVMkiSpERYzSZKkRljMJEmSGmExkyRJaoTFTJIkqREWM0mSpEZYzCRJkhphMZMkSWqExUySJKkRFjNJkqRGNFfMktw/ySeSnJvk7CSHjXsmSZKkITRXzIDXAK+sqvsDv9c9liRJmngtFrMCDujuHwhcNsZZJEmSBpOqGvcM3yfJfYEPAWFUHB9SVV9eZr3NwGaA6enpmbm5ud5nW1hYYGpqyoyGciYlY6icSckYKsd9aS9jqJxJyRgqZ1IyhsqZnZ3dWlWbll1YVYPfgNOBbcvcjgb+HHhSt96TgdN3t72ZmZkawvz8vBmN5UxKxlA5k5IxVI770l7GUDmTkjFUzqRkDJUDnF0rdJqNvVbCFVTVUSstS/J24Lju4d8DbxpkKEmSpDFr8Rqzy4Cf7u4fAVw4xlkkSZIGM5YjZrvxa8Drk2wEvk13HZkkSdKka66YVdXHgJlxzyFJkjS0Fk9lSpIkrUsWM0mSpEZYzCRJkhphMZMkSWqExUySJKkRFjNJkqRGWMwkSZIaYTGTJElqhMVMkiSpERYzSZKkRljMJEmSGmExkyRJaoTFTJIkqREWM0mSpEZYzCRJkhphMZMkSWqExUySJKkRFjNJkqRGWMwkSZIa0VwxS3Joko8n+WySf0xywLhnkiRJGkJzxQx4E/CSqvox4BTgRWOeR5IkaRAtFrN7AWd0908DnjTGWSRJkgaTqhr3DN8nyZnAa6rq/Ul+E3hlVd1mmfU2A5sBpqenZ+bm5nqfbWFhgampKTMaypmUjKFyJiVjqBz3pb2MoXImJWOonEnJGCpndnZ2a1VtWnZhVQ1+A04Hti1zOxq4D/BhYCvwcuC/dre9mZmZGsL8/LwZjeVMSsZQOZOSMVSO+9JexlA5k5IxVM6kZAyVA5xdK3Sajb1WwhVU1VG7WeWRAEnuBfxM/xNJkiSNX3PXmCW5U/fnXsDLgL8c70SSJEnDaK6YAU9N8nngAuAy4K1jnkeSJGkQYzmVuStV9Xrg9eOeQ5IkaWgtHjGTJElalyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJkiQ1wmImSZLUCIuZJElSIyxmkiRJjbCYSZIkNcJiJkmS1AiLmSRJUiMsZpIkSY0YSzFL8vNJzktyQ5JNS5a9NMkXknwuyaPGMZ8kSdI4bBxT7jbgicBfLX4yyf2ApwD/C7gLcHqSe1XVjuFHlCRJGtZYjphV1flV9bllFh0NzFXVd6rqIuALwGHDTidJkjQe4zpitpK7Ap9Y9PiS7rkbSbIZ2Nw9XEiyXNFba3cArjKjqZxJyRgqZ1IyhspxX9rLGCpnUjKGypmUjKFyfmilBb0VsySnAz+wzKLjq+rUW7r9qjoJOOmWbuemSHJ2VW3a/ZpmDJUzKRlD5UxKxlA57kt7GUPlTErGUDmTkjFkzkp6K2ZVddTN+LJLgYMXPb5b95wkSdLEa+3tMv4BeEqSWyW5B3BP4JNjnkmSJGkQ43q7jJ9LcgnwYOCfknwIoKrOA94N/AfwQeC5jb0ic4hTp5OSMVTOpGQMlTMpGUPluC/tZQyVMykZQ+VMSsaQOctKVY0zX5IkSZ3WTmVKkiStWxYzSZKkRljMdmMcHx+V5NAkH0/y2ST/mOSAtdr2ooz7J/lEknOTnJ1kzd/IN8m7uu2fm+TiJOeudcairOcnuaD7Wb2mh+2/Ismli/bnsWudsSTvt5JUkjv0sO1XJflMtx8fTnKXHjJe2/08PpPklCS3XeuMLmfF38812Paju9/tLyR5yVpue1HGW5JcmWRbT9s/OMl8kv/ovk/H9ZSzb5JPJvl0l/PKPnK6rA1JPpXkAz1mXNz9/XtukrN7yrhtkvd0vyfnJ3nwGm//3ov+vjo3yTVJXrCWGYuyfqP7uW9L8s4k+/aQcVy3/fPWcj+W+x1MclCS05Jc2P15u7XKW5Wq8raLG3Bf4N7AFmDToufvB3wauBVwD+CLwIY1yvx34Ke7+78KvKqH/fow8Jju/mOBLT1/H/8E+L2etj0LnA7cqnt8px4yXgG8sM/v0aKsg4EPAV8G7tDD9g9YdP//AH/ZQ8YjgY3d/T8C/qin79Wyv59rsN0N3e/0DwP7dL/r9+th/ocDDwS29fT9uTPwwO7+bYDP97QfAaa6+3sDZwEP6mmffhP4O+ADfWy/y7i4j9+9JRlvA/53d38f4LY9Zm0ArgB+qIdt3xW4CNive/xu4Ng1zjiE0Uc53prR23ydDvzoGm37Rr+DwGuAl3T3X9LX318r3Txiths1no+PuhdwRnf/NOBJa7TdxQrYeSTuQOCyHjIASBLgycA7e4p4DnBCVX0HoKqu7ClnKK8DfpvRz2jNVdU1ix7u30dOVX24qr7bPfwEo/ckXHO7+P28pQ4DvlBVX6qq64A5Rr/za6qqzgD+e623u2j7l1fVOd39bwLns8KnqdzCnKqqhe7h3t1tzf+7SnI34GeAN631toeU5EBGheDNAFV1XVV9vcfII4EvVtWXe9r+RmC/JBsZlae1/v/JfYGzqupb3d8rH2X0edu32Aq/g0czKs50f/7sWmStlsXs5rsr8JVFj1f8+Kib4Tz+538CP8/3v+nuWnkB8NokXwH+GHhpDxk7/RTw1aq6sKft3wv4qSRnJflokp/oKed53am5t/R1aDvJ0cClVfXpPra/KOfV3c/+l4Df6zOL0VHff+k5Y631+fs9FknuDjyA0dGsPra/obtc4UrgtKrqI+fPGP2j5YYetr1YAR9OsjWjj/9ba/cAvga8tTst+6Yk+/eQs9NT6OkfxlV1KaP/h/wncDnwjar68BrHbGP0d/ztk9ya0VmePv6/uNN0VV3e3b8CmO4x60Za+6zMsUjPHx91UzMZ/Y/sz5P8LqM33b2uh4wjgd+oqvcmeTKjf7nd5E9rWOX37qncwr8UdrMvG4GDgAcBPwG8O8kPV3cceo0yTgRexegv7FcxOjX7qzdl+6vM+R1GpwFvkd39XKrqeOD4JC8Fnge8fK0zunWOB74LnHxTt39TcrRrSaaA9wIvWHLEdM3U6D0n799dT3hKkkOqas2unUvyOODKqtqa5PC12u4KHlZVlya5E3Bakgu6IytrZSOj02fPr6qzkrye0Smz313DDACS7AM8gZ7+8d39I/VoRmXz68DfJ3laVb1jrTKq6vwkf8ToEpztwLnAIO9xWlWVZND3FbOYMZ6Pj1pF5iMBktyL0aH7m2xXGUneDuy8EPjvuZmnBna3H92h7ScCMzdn+6vJSfIc4H1dEftkkhsYfQjt19YqY0neXwM3+8LjlXKS/Bijv9w+PTr7y92Ac5IcVlVXrEXGMk4G/pmbUcxW8bM/FngccORNLck3JacnE/PxcEn2ZlTKTq6q9/WdV1VfTzIPPJrRkY618lDgCRm98GZf4IAk76iqp61hBvC9o0BU1ZVJTmF0ansti9klwCWLjiq+h1Ex68NjgHOq6qs9bf8o4KKq+hpAkvcBDwHWrJgBVNWb6U79JvkDRt/Dvnw1yZ2r6vIkd2Z0FHgwnsq8+Xr7+KjuX2kk2Qt4GfCXa7HdJS4Dfrq7fwTQ12nGo4ALqqrPX6L3M3oBwM4iuw9w1VoGdL+cO/0ca/s/HACq6rNVdaequntV3Z3RXzwPvKmlbHeS3HPRw6OBC9Zy+13GoxmdcnpCVX1rrbc/gH8H7pnkHt0Rh6cw+p3fo3TXd74ZOL+q/rTHnDt2R8pIsh/wCNb4v6uqemlV3a373XgK8P/6KGVJ9k9ym533Gf0jeU1/37vf6a8kuXf31JGMPvGmD7f4jMVu/CfwoCS37v57O5LRtYxratH/F3+Q0T/2/26tMxb5B+BXuvu/Agx7ZH7IVxrsiTdG/xO+BPgO8FXgQ4uWHc/olVufo3uF4xplHsfo1VOfB06g+4SGNd6vhwFbGb3a7Cxgpqfv398Az+75Z7QPo3+dbQPOAY7oIeNvgc8Cn2H0S3vnAf7bu5h+XpX53u579RngH4G79pDxBUbXaJ3b3db8lZ9dzoq/n2uw7cd2v4NfZHTatI/538noupzru/145hpv/2GMTr9/ZtHP4rE97MePA5/qcrbR0yuwF+UdTk+vymT0StxPd7fzevzZ3x84u/uevR+4XQ8Z+wP/BRzY88/jlYyK+Lbu78pb9ZDxr4zK66cZHYVfq+3e6HcQuD3wEUYHLE4HDurz+7f05kcySZIkNcJTmZIkSY2wmEmSJDXCYiZJktQIi5kkSVIjLGaSJEmNsJhJWleSHJ/kvO7jtc5N8pNJXtB91MvuvnZV60nSzeXbZUhaN5I8GPhT4PCq+k6SOzB6H7wzgU1Vtcs3Jk5y8WrWk6SbyyNm0v/f3t27RhFFYRh/XoQoJorYSOwEP8BKEEEQCxvBXoR0VilTCWls9F+wEKzsJAjWgoURDLEQBW1E0ohgJQbRFIJyLOaCQ9hKWB12nl955+xwp1lezgz3aEwWgc9V9QOgBayrwFHgaRslRJK7SV62ztqttrYyoe5yks0kr5I8bDMpJemv2TGTNBotOD0H9tOd6L1WVc92d8KSHK6qL0n20J0AvlJVb/p1rdv2iG7qx06SVboTz2//h0eTNCMcYi5pNKrqe5KzwEW6+aprSSYNj76WZJnuP3IROE03OqfvfFvfaEPn54DNae1d0jgYzCSNSlX9AtaB9SRv+TOsGIAkx4AbwLmq2k5yH9g34VYBnlTV0nR3LGlM/MZM0mgkOZXkRG/pDPAB+AYcaGsHgR3ga5IjwJVefb/uBXAhyfF27/kkJ6e5f0mzz46ZpDFZAO4kOQT8BLaAZWAJeJzkU1VdSvIaeAd8BDZ6v7+3q+468CDJ3nb9JvD+Hz2LpBnkx/+SJEkD4atMSZKkgTCYSZIkDYTBTJIkaSAMZpIkSQNhMJMkSRoIg5kkSdJAGMwkSZIG4jejL1Ku0jbvmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, space[policy(space.unsqueeze(1)).argmax(dim=1)], 'p')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Action Chosen by Policy\")\n",
    "plt.title(\"Policy Max Probability Outputs\")\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_optimized = create_trajectories_optimized(1, 30, policy, 1, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.],\n",
       "         [ 4.],\n",
       "         [ 7.],\n",
       "         [-3.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_optimized['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.],\n",
       "         [ 6.],\n",
       "         [10.],\n",
       "         [-4.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.],\n",
       "         [-7.],\n",
       "         [ 7.]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_optimized['states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.],\n",
       "         [-4.],\n",
       "         [-9.],\n",
       "         [-1.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.],\n",
       "         [-0.]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_optimized['rewards']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causality is a fundamental property of our universe. That sounds like a grand statement but is fundamental in physics, for example.\n",
    "\n",
    "What does causality mean? Very simply, an event at a time $t$ can only affect events at future times $t' > t$. There are more precise statements specifically taking into account that events take place at points in space $\\vec{x}$ too and that causal signals can only travel at or below the speed of light in vacuum, $c$.\n",
    "\n",
    "Why are we even talking about this? Look at the expected reward that we are maximizing:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} R(\\tau^{(i)}) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t^{(i)}|s_t^{(i)})\\right)$$\n",
    "\n",
    "Let's look at the contribution of one trajectory so we can drop all the extra indices:\n",
    "\n",
    "$$R(\\tau) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "Recall that the total reward is just the sum of immediate rewards:\n",
    "\n",
    "$$R(\\tau) = \\Sigma_{t=1}^{T} r_t$$\n",
    "\n",
    "and the contribution is:\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} r_u\\right) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "Note that, to prevent confusion, I switched the dummy index on the first sum from $t$ to $u$. This is equivalent to using a dummy index in a for loop - it doesn't matter what you call it. While I could make my case using just manipulations of the above, I'll be pedantic hopefully for the sake of clarity:\n",
    "\n",
    "$$\\left(r_1 + r_2 + \\ldots + r_T\\right) \\left(\\log \\pi(a_1|s_1) + \\log \\pi(a_2|s_2) + \\ldots + \\log \\pi(a_T|s_T)\\right)$$\n",
    "\n",
    "Recall, that we apply a weight of $R(\\tau)$ to **each** action in that trajectory i.e. to each term $\\log\\pi(a_t|s_t)$. We justified this earlier by saying that actions in high-reward trajectories should have a greater say in what gradient we should use.\n",
    "\n",
    "Causality dictates though that an action $a_t$ can only affect current and future rewards, $r_u$ for $u\\geq t$. We shouldn't give $a_t$ credit for any rewards accumulated in this trajectory **before** time $t$. \n",
    "\n",
    "But if we expand the multiplication above, we get:\n",
    "\n",
    "$\\log\\pi(a_1|s_1) \\left(r_1+\\ldots+r_T\\right) + \\ldots + \\log\\pi(a_t|s_t) \\left(r_1+\\ldots+r_T\\right) + \\ldots + \\log\\pi(a_T|s_T) \\left(r_1+\\ldots+r_T\\right)$\n",
    "\n",
    "The generic term is:\n",
    "\n",
    "$$\\log\\pi(a_t|s_t) \\left(r_1+\\ldots+r_T\\right)$$\n",
    "\n",
    "which should really be truncated to:\n",
    "\n",
    "$$\\log\\pi(a_t|s_t) \\left(r_t + r_{t+1} + \\ldots+r_T\\right)$$\n",
    "\n",
    "Note the sum of rewards only considers what occurs at and after time $t$ when the action is taken. In the extreme case when $t=1$, the action should get credit for the full trajectory and in the extreme case when $t=T$, the action should only get credit for the last immediate reward. So, we should replace:\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} r_u\\right) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\\left(\\Sigma_{t=1}^{T} \\left(\\Sigma_{u=1}^{T} r_u\\right) \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "(moving the reward sum within the sum of log probabilities)\n",
    "\n",
    "by \n",
    "\n",
    "$$\\left(\\Sigma_{t=1}^{T} \\left(\\Sigma_{u=\\color{red}t}^{T} r_u\\right) \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "One could also look at this idea from another viewpoint. The above view states that every action should only get credit for rewards in the present and the future. One could also say that every reward only \"touches\" actions in the present and past. A reward at time $t$ should only be multipled by actions at present or in the past. This would suggest replacing:\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} r_u\\right) \\left(\\Sigma_{v=1}^{T} \\log \\pi(a_v|s_v)\\right)$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} \\left(\\Sigma_{v=1}^{T} \\log \\pi(a_v|s_v)\\right) r_u\\right)$$\n",
    "\n",
    "(moving the log probability sum within the sum of rewards)\n",
    "\n",
    "by\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} \\left(\\Sigma_{v=1}^{\\color{red}t} \\log \\pi(a_v|s_v)\\right) r_u\\right)$$\n",
    "\n",
    "So we have two alternatives:\n",
    "\n",
    "$$\\left(\\Sigma_{t=1}^{T} \\left(\\Sigma_{u=\\color{red}t}^{T} r_u\\right) \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\left(\\Sigma_{u=1}^{T} \\left(\\Sigma_{v=1}^{\\color{red}t} \\log \\pi(a_v|s_v)\\right) r_u\\right)$$\n",
    "\n",
    "Which one is the correct one? They are both equivalent! Let's make that obvious by writing both out. Denote $\\log\\pi(a_t|s_t)$ by $\\delta_t$.\n",
    "\n",
    "Start with the first expression:\n",
    "\n",
    "$$\\left(\\Sigma_{t=1}^{T} \\left(\\Sigma_{u=\\color{red}t}^{T} r_u\\right) \\log \\pi(a_t|s_t)\\right)$$\n",
    "\n",
    "Expanding,\n",
    "\n",
    "$$(r_1 + \\ldots + r_T) \\delta_1 + (r_2 + \\ldots + r_{T}) \\delta_2 + \\ldots + (r_{T-1} + r_T) \\delta_{T-1}+ r_T \\delta_T $$\n",
    "\n",
    "Collect all the $r_t$ terms together\n",
    "\n",
    "$$r_1 \\delta_1 + r_2 (\\delta_1 + \\delta_2) + \\ldots + r_T (\\delta_1 + \\delta_2 + \\ldots + \\delta_T)$$\n",
    "\n",
    "FIX INDICES AND MAKE ARGUMENT CLEANER\n",
    "\n",
    "ARGUMENT ABOUT REDUCED VARIANCE\n",
    "\n",
    "To implement this, we have to modify the function expected_reward. Let's generate two trajectories first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_trajectories(2, 10, policy, 1, space, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['action_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rewards'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.from_numpy(np.cumsum(data['rewards'][0][::-1])[::-1].copy()) * torch.cat(data['action_probs'][0]).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_reward(s, causal=False, debug=False):\n",
    "    #TODO: use torch tensor operations\n",
    "    action_probs = s['action_probs']\n",
    "    total_rewards = s['total_rewards']\n",
    "\n",
    "    total = 0\n",
    "    for idx, traj_reward in enumerate(total_rewards): #loop over every trajectory\n",
    "        traj_action_probs = s['action_probs'][idx] #get action probabilities\n",
    "        traj_rewards_list = s['rewards'][idx] #get immediate rewards\n",
    "        \n",
    "        if debug:\n",
    "            print(traj_reward)\n",
    "            print(traj_rewards_list)\n",
    "            print(traj_action_probs)\n",
    "\n",
    "        #causal calculation\n",
    "        if causal:\n",
    "            rewards_to_go = np.cumsum(traj_rewards_list[::-1])[::-1]\n",
    "            total += (torch.from_numpy(rewards_to_go.copy()) * torch.cat(traj_action_probs).log()).sum()\n",
    "        else:\n",
    "            sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities\n",
    "            total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs) \n",
    "        \n",
    "        #for t in range(len(traj_action_probs)):            \n",
    "        #    reward_to_go = np.sum(traj_rewards_list[t:])\n",
    "        #    log_prob = traj_action_probs[t].log()\n",
    "            \n",
    "        #    total += log_prob * reward_to_go\n",
    "            \n",
    "        #sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities\n",
    "        #total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs)\n",
    "\n",
    "    return total / len(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1,\n",
    "                  causal=False,\n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter):\n",
    "    \n",
    "        #step 1: generate batch_size trajectories\n",
    "        s = create_trajectories(batch_size, T, policy, step_size, space, debug=debug)\n",
    "    \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward(s, causal=causal, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "\n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 40000\n",
    "batch_size = 10\n",
    "T = 10\n",
    "\n",
    "policy, exp_reward_list = training_loop(N_iter, \n",
    "                                        batch_size, \n",
    "                                        T, \n",
    "                                        space, \n",
    "                                        policy=policy, \n",
    "                                        lr=1e-2, \n",
    "                                        step_size=1,\n",
    "                                        causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, space[policy(space.unsqueeze(1)).argmax(dim=1)], 'p')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Action Chosen by Policy\")\n",
    "plt.title(\"Policy Max Probability Outputs\")\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy + No causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT MATERIAL\n",
    "\n",
    "Adam blowing up -> epsilon decay since picking epsilon greedy actions with small prob -> tiny gradients\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. random init policy - saturates naturally?\n",
    "2. epsilon greedy decay - encourage wild exploration early?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectories(N, T, policy, step_size, space, epsilon=None, debug=False):\n",
    "    traj_action_probs = []\n",
    "    traj_states = []\n",
    "    traj_rewards = []\n",
    "    traj_total_rewards = []\n",
    "\n",
    "    if epsilon:\n",
    "        assert(0 <= epsilon <= 1), \"0 <= epsilon <= 1 Not Satisfied\"\n",
    "    \n",
    "    \n",
    "    for i in range(N): #loop over trajectories\n",
    "        \n",
    "        #select initial state for trajectory\n",
    "        state = select_start_state(space).unsqueeze(0)\n",
    "        \n",
    "        #needed quantities\n",
    "        action_probs_list = [] #log probability of each action taken\n",
    "        reward_list = [] #each immediate reward received\n",
    "        state_list = [] #each state\n",
    "        total_reward = 0 #total reward\n",
    "\n",
    "        if debug:\n",
    "            print(f'Starting State: {state}')\n",
    "\n",
    "        previous = None\n",
    "        for t in range(T): #loop over time-steps within a trajectory\n",
    "            \n",
    "            #use policy to compute probability distribution on action space\n",
    "            action_probs = policy(state).squeeze(0)\n",
    "            \n",
    "            residual = (action_probs.sum().item()-1)**2\n",
    "            #print(residual)\n",
    "            if not residual < 0.001:\n",
    "                print(t)\n",
    "                print(state)\n",
    "                print(action_probs)\n",
    "                print(previous)\n",
    "                return policy, policy\n",
    "            assert(residual < 0.001)\n",
    "            \n",
    "            #epsilon-greedy\n",
    "            if epsilon:\n",
    "                #with prob epsilon, choose a random action\n",
    "                if np.random.random() < epsilon:                \n",
    "                    previous = 'epsilon'\n",
    "                    action_selected_index = torch.multinomial(torch.ones_like(action_probs), 1) #index in action list\n",
    "                #with prob 1-epsilon, choose according to the probability distribution\n",
    "                else:\n",
    "                    previous = 'normal'\n",
    "                    action_selected_index = torch.multinomial(action_probs, 1) #index in action list\n",
    "                            \n",
    "            #non-epsilon greedy\n",
    "            else:\n",
    "                #sample from the actual distirbution of probabilities\n",
    "                previous = 'normal'\n",
    "                action_selected_index = torch.multinomial(action_probs, 1) #index in action list\n",
    "            \n",
    "            action_selected_prob = action_probs[action_selected_index] #probability value\n",
    "            action_selected = space[action_selected_index] #action selected (in our action space)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'---------Time step: {t}---------')\n",
    "                print(f'Action probabilities: {action_probs}')\n",
    "                print(f'Sum of Action probabilities: {action_probs.sum().item()}')\n",
    "                print(f'Action selected index: {action_selected_index}')\n",
    "                print(f'Action selected prob : {action_selected_prob}')\n",
    "                print(f'Action selected      : {action_selected}')\n",
    "\n",
    "            #record action prob (needed for computing gradients)\n",
    "            action_probs_list.append(action_selected_prob)\n",
    "            \n",
    "            #get immediate reward\n",
    "            r = reward(state, action_selected).squeeze(0)\n",
    "            reward_list.append(r)\n",
    "            \n",
    "            if debug:\n",
    "                print(f'Reward for State {state} Action {action_selected}: {r}')\n",
    "\n",
    "            #use dynamics to jump to next state\n",
    "            state_list.append(state)\n",
    "            state = dynamics(state, action_selected, step_size, space)\n",
    "\n",
    "            if debug:\n",
    "                print(f'New State: {state}')\n",
    "            #add check: if state not in space\n",
    "\n",
    "            #total reward received till now\n",
    "            total_reward += r\n",
    "            \n",
    "            if debug:\n",
    "                print(f'Total reward accumulated: {total_reward}')\n",
    "                print('---------------\\n')\n",
    "\n",
    "        #get data for each trajectory\n",
    "        traj_action_probs.append(action_probs_list)\n",
    "        traj_rewards.append(reward_list)\n",
    "        traj_states.append(state_list)\n",
    "        traj_total_rewards.append(total_reward)\n",
    "\n",
    "    return {\n",
    "            'action_probs': traj_action_probs,\n",
    "            'rewards': traj_rewards,\n",
    "            'states': traj_states,\n",
    "            'total_rewards': traj_total_rewards\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1,\n",
    "                  causal=False,\n",
    "                  epsilon=None,\n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter):\n",
    "        #step 1: generate batch_size trajectories\n",
    "        epsilon = 0.9*epsilon\n",
    "        s = create_trajectories(batch_size, T, policy, step_size, space, epsilon=epsilon, debug=debug)\n",
    "    \n",
    "        if not isinstance(s, dict):\n",
    "            print(f'Training Iter = {i}')\n",
    "            return s[0], s[1]\n",
    "        \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward(s, causal=causal, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        if debug:\n",
    "            print(f'Expected Reward = {exp_reward}')\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "\n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 10000\n",
    "batch_size = 10\n",
    "T = 10\n",
    "\n",
    "policy, exp_reward_list = training_loop(N_iter, \n",
    "                                        batch_size, \n",
    "                                        T, \n",
    "                                        space, \n",
    "                                        policy=policy, \n",
    "                                        lr=1e-2, \n",
    "                                        step_size=1,\n",
    "                                        causal=False,\n",
    "                                        epsilon=0.1,\n",
    "                                        debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, space[policy(space.unsqueeze(1)).argmax(dim=1)], 'p')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Action Chosen by Policy\")\n",
    "plt.title(\"Policy Max Probability Outputs\")\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments so far, recall we estimate the gradient of the expected reward, $J(\\theta)$ as:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau} R(\\tau) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t|s_t)\\right) \\approx \\frac{1}{n}\\Sigma_{i=1}^{n} R(\\tau^{(i)}) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t^{(i)}|s_t^{(i)})\\right)$$\n",
    "\n",
    "Each action is weighed by the total reward of the trajectory it was a part of. Also, recall that the reward structure or function we picked was:\n",
    "\n",
    "$$r(s,a) = -(s-a)^2$$\n",
    "\n",
    "with total reward:\n",
    "\n",
    "$$R(\\tau) = \\Sigma_{t=1}^T r(s_t,a_t)$$\n",
    "\n",
    "Suppose we got very lucky. The initial policy happened to generate a trajectory that had the highest possible reward where each $s_t = a_t$. The total reward in this case would be $R = 0$. This should ideally have a big impact on the policy's weights. The contribution of all the actions in this trajectory would be: \n",
    "\n",
    "$$(R) \\left(\\Sigma_{t=1}^{T} \\log \\pi(a_t^|s_t)\\right)$$\n",
    "\n",
    "which would be: **0**!!! This is because our highest reward which should be desirable has the numerical value 0! In other words, even if we somehow hit upon the best actions for each state in a trajectory, it would make no contribution to the gradients and hence not affect the policy! All other trajectories would have negative rewards and the overall gradient vector would be \"pushed away from\" the gradients contributed by the negative-reward trajectories.\n",
    "\n",
    "What should we do? Should we just add a constant to all the total rewards, $R(\\tau)$ so that the highest reward becomes positive? What should that constant be? Should we instead do something empirical like subtracting the mean reward across trajectories (in our sample)? Let's try this to see what happens.\n",
    "\n",
    "BREAK POINT\n",
    "We have implemented a simple policy gradient algorithm so far. This implementation has high variance i.e. the Monte-Carlo estimate of the gradients of the expected reward, $\\nabla J(\\theta)$ by virtue of using a finite sample of trajectories, results in a quantity that while on average unbiased, has high variance (or fluctuations) from the true expected reward (if one were to use all the trajectories - something that is computationally infeasible).\n",
    "\n",
    "LATER TO DO: evidence of high variance and improvement with causality and baselines\n",
    "\n",
    "One way to reduce the variance was to impose causality, the idea that actions should only be weighted by rewards accumulated after the action was taken since those are the rewards affected by it.\n",
    "\n",
    "Another way to do so is using the idea of **baselines**.\n",
    "\n",
    "So far, the structure of form of the reward, $r_t = (s_t - a_t)^2$ was imposed upon us. Well, not quite, we selected it but presumably because it captures some essence of the behavior we desire from our policy. A natural question is to ask what would happen if we were to transform these rewards in some manner. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i-10 for i in data['rewards'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_reward(s, subtract_mean=False, causal=False, debug=False):\n",
    "    #TODO: use torch tensor operations\n",
    "    action_probs = s['action_probs']\n",
    "    total_rewards = s['total_rewards']\n",
    "    \n",
    "    if len(action_probs)==0:\n",
    "        raise ValueError(\"No data in expected_reward\")\n",
    "\n",
    "    #subtract mean reward from each reward value\n",
    "    mean_reward_per_traj = np.mean(total_rewards)\n",
    "    mean_reward_per_timestep = mean_reward_per_traj / len(action_probs[0])\n",
    "    \n",
    "    total = 0\n",
    "    for idx in range(len(total_rewards)): #loop over every trajectory\n",
    "        traj_action_probs = s['action_probs'][idx] #get action probabilities\n",
    "        traj_rewards_list = s['rewards'][idx] #get immediate rewards\n",
    "        \n",
    "        if subtract_mean:\n",
    "            traj_rewards_list = [(r - mean_reward_per_timestep) for r in traj_rewards_list]\n",
    "        \n",
    "        traj_reward = np.sum(traj_rewards_list)\n",
    "        \n",
    "        if debug:\n",
    "            print(traj_reward)\n",
    "            print(traj_rewards_list)\n",
    "            print(traj_action_probs)\n",
    "\n",
    "        #causal calculation\n",
    "        if causal:\n",
    "            rewards_to_go = np.cumsum(traj_rewards_list[::-1])[::-1]\n",
    "            total += (torch.from_numpy(rewards_to_go.copy()) * torch.cat(traj_action_probs).log()).sum()\n",
    "        else:\n",
    "            sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities\n",
    "            total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs) \n",
    "        \n",
    "        #for t in range(len(traj_action_probs)):            \n",
    "        #    reward_to_go = np.sum(traj_rewards_list[t:])\n",
    "        #    log_prob = traj_action_probs[t].log()\n",
    "            \n",
    "        #    total += log_prob * reward_to_go\n",
    "            \n",
    "        #sum_log_prob = torch.cat(traj_action_probs).log().sum() #sum of log probabilities\n",
    "        #total += sum_log_prob * traj_reward #J = (total reward)*(sum of log probs)\n",
    "\n",
    "    return total / len(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(N_iter, \n",
    "                  batch_size, \n",
    "                  T, \n",
    "                  space, \n",
    "                  policy=None, \n",
    "                  lr=1e-2, \n",
    "                  step_size=1,\n",
    "                  subtract_mean=False,\n",
    "                  causal=False,\n",
    "                  epsilon=None,\n",
    "                  debug=False):\n",
    "    \n",
    "    if policy is None:\n",
    "        policy = PolicyNet(1, len(space), 1, 10, nn.ReLU(), nn.Softmax(dim=1))\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    exp_reward_list = []\n",
    "\n",
    "    for i in range(N_iter):\n",
    "        #step 1: generate batch_size trajectories\n",
    "        s = create_trajectories(batch_size, T, policy, step_size, space, epsilon=epsilon, debug=debug)\n",
    "    \n",
    "        if not isinstance(s, dict):\n",
    "            print(f'Training Iter = {i}')\n",
    "            return s[0], s[1]\n",
    "        \n",
    "        #step 2: define J\n",
    "        exp_reward = expected_reward(s, subtract_mean=subtract_mean, causal=causal, debug=debug)\n",
    "        exp_reward_list.append(exp_reward)\n",
    "\n",
    "        if debug:\n",
    "            print(f'Expected Reward = {exp_reward}')\n",
    "        #step 3: do gradient ascent\n",
    "        optimize(optimizer, exp_reward, lr=lr)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            #if np.mean(s[\"total_rewards\"]) > -40:\n",
    "            #    print(f'{np.mean(s[\"total_rewards\"])}')\n",
    "            #    return policy, exp_reward_list\n",
    "            print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "\n",
    "    print(f'Iter {i}: Expected Reward = {exp_reward} Empirical Reward = {np.mean(s[\"total_rewards\"])}')\n",
    "            \n",
    "    return policy, exp_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize policy\n",
    "N_inputs = 1 #1 number for your current state since space is 1-dimensional\n",
    "N_outputs = len(space) #probability for each action\n",
    "N_hidden_layers = 1\n",
    "N_hidden_nodes = 10\n",
    "activation = nn.ReLU() #activation in hidden layers\n",
    "output_activation = nn.Softmax(dim=1) #want probability distribution on action space\n",
    "#technically, we should not use Sigmoid since we only care about log probs\n",
    "#but don't worry about this now\n",
    "\n",
    "policy = PolicyNet(N_inputs, \n",
    "                   N_outputs, \n",
    "                   N_hidden_layers, \n",
    "                   N_hidden_nodes,\n",
    "                   activation,\n",
    "                   output_activation=output_activation)\n",
    "\n",
    "N_iter = 10000\n",
    "batch_size = 10\n",
    "T = 10\n",
    "\n",
    "policy, exp_reward_list = training_loop(N_iter, \n",
    "                                        batch_size, \n",
    "                                        T, \n",
    "                                        space, \n",
    "                                        policy=policy, \n",
    "                                        lr=1e-2, \n",
    "                                        step_size=1,\n",
    "                                        subtract_mean=True,\n",
    "                                        causal=False,\n",
    "                                        epsilon=None,\n",
    "                                        debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_trajectories(1, 10, policy, 1, space, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(space, space[policy(space.unsqueeze(1)).argmax(dim=1)], 'p')\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Action Chosen by Policy\")\n",
    "plt.title(\"Policy Max Probability Outputs\")\n",
    "_ = plt.xticks(space)\n",
    "_ = plt.yticks(space)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Aside on Concentration Inequalities\n",
    "\n",
    "(Will add this in small pieces over time - not a priority given above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While discussing Monte-Carlo, we mentioned that we wanted to estimate the mean of a quantity (the gradient) by sampling $n$ values and taking their empirical mean.\n",
    "\n",
    "This problem can be formalized as:\n",
    "\n",
    "Given $n$ i.i.d. (independent and identically distributed) random variables $X_1, X_2, \\ldots, X_n$ with mean $\\mu$ (they are the same since they are i.i.d.), what is the deviation between the empirical mean, $\\hat{\\mu} = \\frac{1}{n} (X_1 + X_2 + \\ldots + X_n)$ and the true mean $\\mu$?\n",
    "\n",
    "As before, you can think of the measuring the average height of people in a country. $X_i$ refers to the height of the $ith$ person, $n$ is the number of people in your sample and since each person is chosen randomly, $X_i$ is drawn from the overall distribution of heights for the population.\n",
    "\n",
    "Alternately, we want to understand and quantify the effect of a finite sample on the error in the empirical mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest attack against this problem is using the Markov inequality. Consider a non-negative random variable, X (it only takes values that are 0 or positive, like heights).\n",
    "\n",
    "We can write the mean as:\n",
    "\n",
    "$$\\mathbb{E}X = \\int_{0}^{\\infty} x \\mathbb{P}(x)dx$$\n",
    "\n",
    "where $\\mathbb{P}$ denotes the probability distribution or p.d.f.\n",
    "\n",
    "So,\n",
    "\n",
    "$$\\mathbb{E}X = \\int_{0}^{\\infty} x \\mathbb{P}(x)dx = \\int_{0}^{a} x \\mathbb{P}(x)dx + \\int_{a}^{\\infty} x \\mathbb{P}(x)dx$$\n",
    "\n",
    "for any $a\\geq0$.\n",
    "\n",
    "Since, $X \\geq 0$ (i.e. it takes non-negative values), each integral is $\\geq0$. So, we can drop one term and focus on the tails:\n",
    "\n",
    "$$\\mathbb{E}X \\geq \\int_{a}^{\\infty} x \\mathbb{P}(x) dx \\geq \\int_{a}^{\\infty} a \\mathbb{P}(x)dx = a \\int_{a}^{\\infty} \\mathbb{P}(x)dx = a \\mathbb{P}(X \\geq a)$$\n",
    "\n",
    "For clarity, let's go through each step. The first inequality results from the fact that the expectation is the sum of two positive terms (the integrals) and hence it is larger than one integral alone. The second inequality results in replacing the $x$ by a smaller term $a$ (the lower limit). The last equality just states that the integral of the p.d.f. from $a$ to $\\infty$ is just the probability that $X$ yields a value in that range i.e. the **tail probability** of $X > a$.\n",
    "\n",
    "$$\\implies \\boxed{\\mathbb{P}(X \\geq a) \\leq \\frac{\\mathbb{E}X}{a}}$$\n",
    "\n",
    "Let's understand the spirit behind this inequality. The tail probability $\\mathbb{P}(X \\geq a)$ is the probability that the non-negative random variable (\"measurement\") $X$ yields a value $\\geq a$. If our goal is to minimize this, then the inequality gives us an upper-bound that can be written in terms of the expected (average) value, $\\mathbb{E}X$ and the value $a$. By minimizing the upper bound, we minimize the tail probability.\n",
    "\n",
    "Every time we get a mathematical result like this, we are obliged to do some sanity checks. Do we agree with this funny looking result?\n",
    "\n",
    "Do the units make sense? The left-hand-side (LHS) is a probability i.e. a dimensionless number. The right-hand-side (RHS) is a ratio of dimension**full** numbers ($X$ might be measured in meters or kilograms or something else) which is also dimensionless.\n",
    "\n",
    "As you keep increasing $a$, the tail keeps getting smaller and thus the tail probability should get smaller. Does that happen? Yes, $\\mathbb{E}X$ is independent of $a$ and as $a$ increases, $\\frac{1}{a}$ goes to 0. More formally, $\\lim_{a\\rightarrow \\infty} \\frac{\\mathbb{E}X}{a} \\rightarrow 0$. This means $\\lim_{a\\rightarrow \\infty} \\mathbb{P}(X\\geq a) \\leq \\lim_{a \\rightarrow \\infty} \\frac{\\mathbb{E}X}{a} = 0$. We know that $P(X \\geq a) \\geq 0$ since it's a probability distribution. So $\\lim_{a\\rightarrow\\infty}\\mathbb{P}(X\\geq a) \\leq 0$ **and** $\\lim_{a\\rightarrow\\infty}\\mathbb{P}(X\\geq a) \\geq 0$ which means $\\lim_{a\\rightarrow\\infty}\\mathbb{P}(X\\geq a) = 0$.\n",
    "\n",
    "INSERT PICTURE\n",
    "\n",
    "Why did we go through this trouble? What does this have to do with our original problem of characterizing deviations between the true mean $\\mu$ and the empirical mean? Doesn't Markov just deal with non-negative random variables. We never said the original $X_i$ are non-negative. Hold on!\n",
    "\n",
    "Suppose we are given a random variable $X$ with true mean $\\mu$. Then we can define deviation to be:\n",
    "\n",
    "$$Y = (X - \\mu)^2$$\n",
    "\n",
    "$Y$ is a non-negative random variable i.e. $Y \\geq 0$. Applying Markov:\n",
    "\n",
    "$$\\mathbb{P}(Y \\geq a^2) \\leq \\frac{\\mathbb{E}Y}{a^2}$$\n",
    "\n",
    "Instead of $a$, I am writing $a^2$ (without any loss of generality but just to make it clear that $Y$ has units of $X^2$). Note that $\\mathbb{E}Y = \\mathbb{E}(X - \\mu)^2$ is precisely the standard deviation of $X$! Denote this by $\\sigma^2$. So,\n",
    "\n",
    "$$\\mathbb{P}((X - \\mu)^2 \\geq a^2) \\leq \\frac{\\sigma^2}{a^2}$$\n",
    "\n",
    "Remember that $X$ is the empirical mean $\\hat{\\mu}$ with true mean $\\mu$. So we can safely write:\n",
    "\n",
    "$$\\boxed{\\mathbb{P}((\\hat{\\mu} - \\mu)^2 \\geq a^2) \\leq \\frac{\\sigma^2}{a^2}}$$\n",
    "\n",
    "Great! We have a bound on the probability that the difference between the empirical mean $\\hat{mu}$ and the true mean $\\mu$ exceeds an amount $a$. The greater the difference $a$, the smaller the probability and the upper bound on the probability decreases as $\\frac{1}{a^2}$. Can we make this better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "k = 1000\n",
    "mean = 0\n",
    "var = (1+1)**2/12.\n",
    "\n",
    "vals = np.array([np.square(np.mean(np.random.uniform(-1,1,m)) - mean) for i in range(k)])\n",
    "n,b,p = plt.hist(vals/var)\n",
    "plt.title(f\"Squared Deviation / Variance for Sample Size = {m} and for {k} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
